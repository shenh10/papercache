llm:
  algorithm:
    RL:
    - categories:
      - llm
      - algorithm
      - RL
      date: '2025-08-01'
      title: 'SeamlessFlow: A Trainerâ€“Agent Isolation RL Framework Achieving Bubble-Free
        Pipelines via Tag Scheduling'
      url: /llm/algorithm/RL/2025/08/01/seamlessflow-a-traineragent-isolation-rl-framework-achieving-bubble-free-pipelines-via-tag-scheduling.html
    - categories:
      - llm
      - algorithm
      - RL
      date: '2025-08-01'
      title: 'ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING
        AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING'
      url: /llm/algorithm/RL/2025/08/01/on-policy-rl-meets-off-policy-experts-harmonizing-supervised-fine-tuning-and-reinforcement-learning-via-dynamic-weighting.html
    - categories:
      - llm
      - algorithm
      - RL
      date: '2024-10-01'
      title: 'HybridFlow: A Flexible and Efficient RLHF Framework'
      url: /llm/algorithm/RL/2024/10/01/hybridflow-a-flexible-and-efficient-rlhf-framework.html
    - categories:
      - llm
      - algorithm
      - RL
      date: '2024-05-01'
      title: 'OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework'
      url: /llm/algorithm/RL/2024/05/01/openrlhf-an-easy-to-use-scalable-and-high-performance-rlhf-framework.html
    - categories:
      - llm
      - algorithm
      - RL
      date: '2024-02-01'
      title: 'DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language
        Models'
      url: /llm/algorithm/RL/2024/02/01/deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models.html
    - categories:
      - llm
      - algorithm
      - RL
      date: '2023-05-01'
      title: 'Direct Preference Optimization: Your Language Model is Secretly a Reward
        Model'
      url: /llm/algorithm/RL/2023/05/01/direct-preference-optimization-your-language-model-is-secretly-a-reward-model.html
    - categories:
      - llm
      - algorithm
      - RL
      date: '2017-08-01'
      title: Proximal Policy Optimization Algorithms
      url: /llm/algorithm/RL/2017/08/01/proximal-policy-optimization-algorithms.html
    agent:
    - categories:
      - llm
      - algorithm
      - agent
      date: '2025-05-01'
      title: 'A Survey on Test-Time Scaling in Large Language Models: What, How, Where,
        and How Well'
      url: /llm/algorithm/agent/2025/05/01/a-survey-on-test-time-scaling-in-large-language-models-what-how-where-and-how-well.html
    - categories:
      - llm
      - algorithm
      - agent
      date: '2025-04-01'
      title: 'Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory'
      url: /llm/algorithm/agent/2025/04/01/mem0-building-production-ready-ai-agents-with-scalable-long-term-memory.html
    - categories:
      - llm
      - algorithm
      - agent
      date: '2025-03-01'
      title: 'Search-R1: Training LLMs to Reason and Leverage Search Engines with
        Reinforcement Learning'
      url: /llm/algorithm/agent/2025/03/01/search-r1-training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning.html
    - categories:
      - llm
      - algorithm
      - agent
      date: '2025-03-01'
      title: 'R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement
        Learning'
      url: /llm/algorithm/agent/2025/03/01/r1-searcher-incentivizing-the-search-capability-in-llms-via-reinforcement-learning.html
    - categories:
      - llm
      - algorithm
      - agent
      date: '2024-09-01'
      title: 'Large Language Model-Based Agents for Software Engineering: A Survey'
      url: /llm/algorithm/agent/2024/09/01/large-language-model-based-agents-for-software-engineering-a-survey.html
    - categories:
      - llm
      - algorithm
      - agent
      date: '2023-10-01'
      title: 'FIREACT: TOWARD LANGUAGE AGENT FINE-TUNING'
      url: /llm/algorithm/agent/2023/10/01/fireact-toward-language-agent-fine-tuning.html
    models:
    - categories:
      - llm
      - algorithm
      - models
      date: '2025-08-01'
      title: 'KIMI K2: OPEN AGENTIC INTELLIGENCE'
      url: /llm/algorithm/models/2025/08/01/kimi-k2-open-agentic-intelligence.html
    - categories:
      - llm
      - algorithm
      - models
      date: '2025-04-01'
      title: 'Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement
        Learning'
      url: /llm/algorithm/models/2025/04/01/seed15-thinking-advancing-superb-reasoning-models-with-reinforcement-learning.html
    - categories:
      - llm
      - algorithm
      - models
      date: '2025-01-01'
      title: 'KIMI K1.5: SCALING REINFORCEMENT LEARNING WITH LLMS'
      url: /llm/algorithm/models/2025/01/01/kimi-k15-scaling-reinforcement-learning-with-llms.html
    - categories:
      - llm
      - algorithm
      - models
      date: '2024-12-01'
      title: DeepSeek-V3 Technical Report
      url: /llm/algorithm/models/2024/12/01/deepseek-v3-technical-report.html
    pretrain:
    - categories:
      - llm
      - algorithm
      - pretrain
      date: '2022-03-01'
      title: "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot\n    \
        \  Hyperparameter Transfer"
      url: /llm/algorithm/pretrain/2022/03/01/tensor-programs-v-tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer.html
    - categories:
      - llm
      - algorithm
      - pretrain
      date: '2021-04-01'
      title: 'ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING'
      url: /llm/algorithm/pretrain/2021/04/01/roformer-enhanced-transformer-with-rotary-position-embedding.html
  engineering:
    attention:
    - categories:
      - llm
      - engineering
      - attention
      date: '2025-05-01'
      title: 'SageAttention3: Microscaling FP4 Attention for Inference and An Exploration
        of 8-bit Training'
      url: /llm/engineering/attention/2025/05/01/sageattention3-microscaling-fp4-attention-for-inference-and-an-exploration-of-8-bit-training.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2025-02-01'
      title: 'MOBA: MIXTURE OF BLOCK ATTENTION FOR LONG-CONTEXT LLMS'
      url: /llm/engineering/attention/2025/02/01/moba-mixture-of-block-attention-for-long-context-llms.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2025-02-01'
      title: 'TREE ATTENTION: TOPOLOGY-AWARE DECODING FOR LONG-CONTEXT ATTENTION ON
        GPU CLUSTERS'
      url: /llm/engineering/attention/2025/02/01/tree-attention-topology-aware-decoding-for-long-context-attention-on-gpu-clusters.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2025-02-01'
      title: 'Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse
        Attention'
      url: /llm/engineering/attention/2025/02/01/native-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2025-01-01'
      title: 'MiniMax-01: Scaling Foundation Models with Lightning Attention'
      url: /llm/engineering/attention/2025/01/01/minimax-01-scaling-foundation-models-with-lightning-attention.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2024-10-01'
      title: 'SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE
        ACCELERATION'
      url: /llm/engineering/attention/2024/10/01/sageattention-accurate-8-bit-attention-for-plug-and-play-inference-acceleration.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2024-07-01'
      title: 'FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision'
      url: /llm/engineering/attention/2024/07/01/flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2024-04-01'
      title: 'Leave No Context Behind: Efficient Infinite Context Transformers with
        Infini-attention'
      url: /llm/engineering/attention/2024/04/01/leave-no-context-behind-efficient-infinite-context-transformers-with-infini-attention.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2023-12-01'
      title: 'GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
        Checkpoints'
      url: /llm/engineering/attention/2023/12/01/gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2023-07-01'
      title: 'FlashAttention-2: Faster Attention with Better Parallelism and Work
        Partitioning'
      url: /llm/engineering/attention/2023/07/01/flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2022-07-01'
      title: 'FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness'
      url: /llm/engineering/attention/2022/07/01/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2020-06-01'
      title: 'Transformers are RNNs: Fast Autoregressive Transformers with Linear
        Attention'
      url: /llm/engineering/attention/2020/06/01/transformers-are-rnns-fast-autoregressive-transformers-with-linear-attention.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2020-02-01'
      title: Low-Rank Bottleneck in Multi-head Attention Models
      url: /llm/engineering/attention/2020/02/01/low-rank-bottleneck-in-multi-head-attention-models.html
    - categories:
      - llm
      - engineering
      - attention
      date: '2019-11-01'
      title: 'Fast Transformer Decoding: One Write-Head is All You Need'
      url: /llm/engineering/attention/2019/11/01/fast-transformer-decoding-one-write-head-is-all-you-need.html
    compiler:
    - categories:
      - llm
      - engineering
      - compiler
      date: '2025-06-01'
      title: 'Mirage: A Multi-Level Superoptimizer for Tensor Programs'
      url: /llm/engineering/compiler/2025/06/01/mirage-a-multi-level-superoptimizer-for-tensor-programs.html
    - categories:
      - llm
      - engineering
      - compiler
      date: '2025-04-01'
      title: 'TileLang: A Composable Tiled Programming Model for AI Systems'
      url: /llm/engineering/compiler/2025/04/01/tilelang-a-composable-tiled-programming-model-for-ai-systems.html
    - categories:
      - llm
      - engineering
      - compiler
      date: '2025-04-01'
      title: 'TileLink: Generating Efficient Compute-Communication Overlapping Kernels
        using Tile-Centric Primitives'
      url: /llm/engineering/compiler/2025/04/01/tilelink-generating-efficient-compute-communication-overlapping-kernels-using-tile-centric-primitives.html
    - categories:
      - llm
      - engineering
      - compiler
      date: '2024-12-01'
      title: 'FLEX ATTENTION: A PROGRAMMING MODEL FOR GENERATING OPTIMIZED ATTENTION
        KERNELS'
      url: /llm/engineering/compiler/2024/12/01/flex-attention-a-programming-model-for-generating-optimized-attention-kernels.html
    - categories:
      - llm
      - engineering
      - compiler
      date: '2024-10-01'
      title: 'FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL
        FUSION'
      url: /llm/engineering/compiler/2024/10/01/flux-fast-software-based-communication-overlap-on-gpus-through-kernel-fusion.html
    inference:
    - categories:
      - llm
      - engineering
      - inference
      date: '2025-07-01'
      title: 'MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated
        Expert Parallelism'
      url: /llm/engineering/inference/2025/07/01/megascale-infer-serving-mixture-of-experts-at-scale-with-disaggregated-expert-parallelism.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2025-07-01'
      title: 'Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective
        Decoding'
      url: /llm/engineering/inference/2025/07/01/step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2025-05-01'
      title: 'Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware
        for AI Architectures'
      url: /llm/engineering/inference/2025/05/01/insights-into-deepseek-v3-scaling-challenges-and-reflections-on-hardware-for-ai-architectures.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2025-04-01'
      title: 'Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation
        in LLM Serving'
      url: /llm/engineering/inference/2025/04/01/tilus-a-virtual-machine-for-arbitrary-low-precision-gpgpu-computation-in-llm-serving.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2024-08-01'
      title: 'NanoFlow: Towards Optimal Large Language Model Serving Throughput'
      url: /llm/engineering/inference/2024/08/01/nanoflow-towards-optimal-large-language-model-serving-throughput.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2024-07-01'
      title: 'Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving'
      url: /llm/engineering/inference/2024/07/01/mooncake-a-kvcache-centric-disaggregated-architecture-for-llm-serving.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2024-07-01'
      title: 'SGLang: Efficient Execution of Structured Language Model Programs'
      url: /llm/engineering/inference/2024/07/01/sglang-efficient-execution-of-structured-language-model-programs.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2024-05-01'
      title: Efficient Heterogeneous Large Language Model Decoding with Model-Attention
        Disaggregation
      url: /llm/engineering/inference/2024/05/01/efficient-heterogeneous-large-language-model-decoding-with-model-attention-disaggregation.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2024-01-01'
      title: 'DistServe: Disaggregating Prefill and Decoding for Goodput-optimized
        Large Language Model Serving'
      url: /llm/engineering/inference/2024/01/01/distserve-disaggregating-prefill-and-decoding-for-goodput-optimized-large-language-model-serving.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2023-09-01'
      title: Efficient Memory Management for Large Language Model Serving with PagedAttention
      url: /llm/engineering/inference/2023/09/01/efficient-memory-management-for-large-language-model-serving-with-pagedattention.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2023-08-01'
      title: 'SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked
        Prefills'
      url: /llm/engineering/inference/2023/08/01/sarathi-efficient-llm-inference-by-piggybacking-decodes-with-chunked-prefills.html
    - categories:
      - llm
      - engineering
      - inference
      date: '2022-07-01'
      title: 'Orca: A Distributed Serving System for Transformer-Based Generative
        Models'
      url: /llm/engineering/inference/2022/07/01/orca-a-distributed-serving-system-for-transformer-based-generative-models.html
    kvcache:
    - categories:
      - llm
      - engineering
      - inference
      - kvcache
      date: '2024-10-01'
      title: Do Large Language Models Need a Content Delivery Network?
      url: /llm/engineering/inference/kvcache/2024/10/01/do-large-language-models-need-a-content-delivery-network.html
    - categories:
      - llm
      - engineering
      - inference
      - kvcache
      date: '2024-07-01'
      title: Cost-Efficient Large Language Model Serving for Multi-turn Conversations
        with CachedAttention
      url: /llm/engineering/inference/kvcache/2024/07/01/cost-efficient-large-language-model-serving-for-multi-turn-conversations-with-cachedattention.html
    - categories:
      - llm
      - engineering
      - inference
      - kvcache
      date: '2024-05-01'
      title: 'CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge
        Fusion'
      url: /llm/engineering/inference/kvcache/2024/05/01/cacheblend-fast-large-language-model-serving-for-rag-with-cached-knowledge-fusion.html
    - categories:
      - llm
      - engineering
      - inference
      - kvcache
      date: '2024-04-01'
      title: 'PROMPT CACHE: MODULAR ATTENTION REUSE FOR LOW-LATENCY INFERENCE'
      url: /llm/engineering/inference/kvcache/2024/04/01/prompt-cache-modular-attention-reuse-for-low-latency-inference.html
    - categories:
      - llm
      - engineering
      - inference
      - kvcache
      date: '2023-10-01'
      title: 'CacheGen: KV Cache Compression and Streaming for Fast Large Language
        Model Serving'
      url: /llm/engineering/inference/kvcache/2023/10/01/cachegen-kv-cache-compression-and-streaming-for-fast-large-language-model-serving.html
    low_precision:
    - categories:
      - llm
      - engineering
      - inference
      - low_precision
      date: '2025-05-01'
      title: Recipes for Pre-training LLMs with MXFP8
      url: /llm/engineering/inference/low_precision/2025/05/01/recipes-for-pre-training-llms-with-mxfp8.html
    - categories:
      - llm
      - engineering
      - inference
      - low_precision
      date: '2024-01-01'
      title: 'FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric
        Algorithm-System Co-Design'
      url: /llm/engineering/inference/low_precision/2024/01/01/fp6-llm-efficiently-serving-large-language-models-through-fp6-centric-algorithm-system-co-design.html
    - categories:
      - llm
      - engineering
      - inference
      - low_precision
      date: '2023-06-01'
      title: 'AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION FOR ON-DEVICE LLM COMPRESSION
        AND ACCELERATION'
      url: /llm/engineering/inference/low_precision/2023/06/01/awq-activation-aware-weight-quantization-for-on-device-llm-compression-and-acceleration.html
    - categories:
      - llm
      - engineering
      - inference
      - low_precision
      date: '2023-06-01'
      title: FP8 versus INT8 for efficient deep learning inference
      url: /llm/engineering/inference/low_precision/2023/06/01/fp8-versus-int8-for-efficient-deep-learning-inference.html
    - categories:
      - llm
      - engineering
      - inference
      - low_precision
      date: '2023-05-01'
      title: Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large
        Language Models
      url: /llm/engineering/inference/low_precision/2023/05/01/integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models.html
    - categories:
      - llm
      - engineering
      - inference
      - low_precision
      date: '2023-04-01'
      title: With Shared Microexponents, A Little Shifting Goes a Long Way
      url: /llm/engineering/inference/low_precision/2023/04/01/with-shared-microexponents-a-little-shifting-goes-a-long-way.html
    - categories:
      - llm
      - engineering
      - inference
      - low_precision
      date: '2022-11-01'
      title: 'SmoothQuant: Accurate and Efficient Post-Training Quantization for Large
        Language Models'
      url: /llm/engineering/inference/low_precision/2022/11/01/smoothquant-accurate-and-efficient-post-training-quantization-for-large-language-models.html
    speculative_decoding:
    - categories:
      - llm
      - engineering
      - inference
      - speculative_decoding
      date: '2025-03-01'
      title: 'EAGLE-3: Scaling up Inference Acceleration of Large Language Models
        via Training-Time Test'
      url: /llm/engineering/inference/speculative_decoding/2025/03/01/eagle-3-scaling-up-inference-acceleration-of-large-language-models-via-training-time-test.html
    - categories:
      - llm
      - engineering
      - inference
      - speculative_decoding
      date: '2024-06-01'
      title: 'MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding
        Heads'
      url: /llm/engineering/inference/speculative_decoding/2024/06/01/medusa-simple-llm-inference-acceleration-framework-with-multiple-decoding-heads.html
    - categories:
      - llm
      - engineering
      - inference
      - speculative_decoding
      date: '2024-06-01'
      title: 'EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees'
      url: /llm/engineering/inference/speculative_decoding/2024/06/01/eagle-2-faster-inference-of-language-models-with-dynamic-draft-trees.html
    - categories:
      - llm
      - engineering
      - inference
      - speculative_decoding
      date: '2024-01-01'
      title: 'EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty'
      url: /llm/engineering/inference/speculative_decoding/2024/01/01/eagle-speculative-sampling-requires-rethinking-feature-uncertainty.html
    train:
    - categories:
      - llm
      - engineering
      - train
      date: '2024-07-01'
      title: 'Efficient Training of Large Language Models on Distributed Infrastructures:
        A Survey'
      url: /llm/engineering/train/2024/07/01/efficient-training-of-large-language-models-on-distributed-infrastructures-a-survey.html
    - categories:
      - llm
      - engineering
      - train
      date: '2024-02-01'
      title: 'MegaScale: Scaling Large Language Model Training to More Than 10,000
        GPUs'
      url: /llm/engineering/train/2024/02/01/megascale-scaling-large-language-model-training-to-more-than-10000-gpus.html
    - categories:
      - llm
      - engineering
      - train
      date: '2023-11-01'
      title: ZERO BUBBLE PIPELINE PARALLELISM
      url: /llm/engineering/train/2023/11/01/zero-bubble-pipeline-parallelism.html
    - categories:
      - llm
      - engineering
      - train
      date: '2023-04-01'
      title: 'PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel'
      url: /llm/engineering/train/2023/04/01/pytorch-fsdp-experiences-on-scaling-fully-sharded-data-parallel.html
    - categories:
      - llm
      - engineering
      - train
      date: '2022-04-01'
      title: 'PaLM: Scaling Language Modeling with Pathways'
      url: /llm/engineering/train/2022/04/01/palm-scaling-language-modeling-with-pathways.html
    - categories:
      - llm
      - engineering
      - train
      date: '2021-08-01'
      title: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
      url: /llm/engineering/train/2021/08/01/efficient-large-scale-language-model-training-on-gpu-clusters-using-megatron-lm.html
    - categories:
      - llm
      - engineering
      - train
      date: '2021-04-01'
      title: 'ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning'
      url: /llm/engineering/train/2021/04/01/zero-infinity-breaking-the-gpu-memory-wall-for-extreme-scale-deep-learning.html
    - categories:
      - llm
      - engineering
      - train
      date: '2021-01-01'
      title: 'ZeRO-Offload: Democratizing Billion-Scale Model Training'
      url: /llm/engineering/train/2021/01/01/zero-offload-democratizing-billion-scale-model-training.html
    - categories:
      - llm
      - engineering
      - train
      date: '2020-03-01'
      title: 'ZeRO: Memory Optimizations Toward Training Trillion Parameter Models'
      url: /llm/engineering/train/2020/03/01/zero-memory-optimizations-toward-training-trillion-parameter-models.html
    - categories:
      - llm
      - engineering
      - train
      date: '2020-03-01'
      title: 'Megatron-LM: Training Multi-Billion Parameter Language Models Using
        Model Parallelism'
      url: /llm/engineering/train/2020/03/01/megatron-lm-training-multi-billion-parameter-language-models-using-model-parallelism.html
    - categories:
      - llm
      - engineering
      - train
      date: '2019-07-01'
      title: 'GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism'
      url: /llm/engineering/train/2019/07/01/gpipe-easy-scaling-with-micro-batch-pipeline-parallelism.html
    - categories:
      - llm
      - engineering
      - train
      date: '2018-06-01'
      title: 'PipeDream: Fast and Efficient Pipeline Parallel DNN Training'
      url: /llm/engineering/train/2018/06/01/pipedream-fast-and-efficient-pipeline-parallel-dnn-training.html
mlsys:
  framework:
    framework:
    - categories:
      - mlsys
      - framework
      date: '2025-08-25'
      title: 'Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural
        Network Training'
      url: /mlsys/framework/2025/08/25/campo-cost-aware-performance-optimization-for-mixed-precision-neural-network-training.html
  gpu:
    gpu:
    - categories:
      - mlsys
      - gpu
      date: '2023-01-01'
      title: 'Stream-K: Work-centric Parallel Decomposition for Dense Matrix-Matrix
        Multiplication on the GPU'
      url: /mlsys/gpu/2023/01/01/stream-k-work-centric-parallel-decomposition-for-dense-matrix-matrix-multiplication-on-the-gpu.html
    - categories:
      - mlsys
      - gpu
      date: '2022-01-01'
      title: NVIDIA H100 Tensor Core GPU Architecture
      url: /mlsys/gpu/2022/01/01/nvidia-h100-tensor-core-gpu-architecture.html
    - categories:
      - mlsys
      - gpu
      date: '2020-02-01'
      title: 'GPU Initiated OpenSHMEM: Correct and Eicient Intra-Kernel Networking
        for dGPUs'
      url: /mlsys/gpu/2020/02/01/gpu-initiated-openshmem-correct-and-eicient-intra-kernel-networking-for-dgpus.html
    - categories:
      - mlsys
      - gpu
      date: '2018-03-01'
      title: Improving Real-Time Performance with CUDA Persistent Threads (CuPer)
        on the Jetson TX2
      url: /mlsys/gpu/2018/03/01/improving-real-time-performance-with-cuda-persistent-threads-cuper-on-the-jetson-tx2.html
    - categories:
      - mlsys
      - gpu
      date: '2017-04-01'
      title: Locality-Aware CTA Clustering for Modern GPUs
      url: /mlsys/gpu/2017/04/01/locality-aware-cta-clustering-for-modern-gpus.html
    - categories:
      - mlsys
      - gpu
      date: '2016-04-01'
      title: Optimizing Performance of Recurrent Neural Networks on GPUs
      url: /mlsys/gpu/2016/04/01/optimizing-performance-of-recurrent-neural-networks-on-gpus.html
    - categories:
      - mlsys
      - gpu
      date: '2010-01-01'
      title: Demystifying GPU Microarchitecture through Microbenchmarking
      url: /mlsys/gpu/2010/01/01/demystifying-gpu-microarchitecture-through-microbenchmarking.html
  networking:
    networking:
    - categories:
      - mlsys
      - networking
      date: '2025-07-01'
      title: "Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols\
        \ and\n      Algorithms"
      url: /mlsys/networking/2025/07/01/demystifying-nccl-an-in-depth-analysis-of-gpu-communication-protocols-and-algorithms.html
    - categories:
      - mlsys
      - networking
      date: '2025-07-01'
      title: Scale-Up Ethernet Framework Scale-Up Ethernet Framework Specification
      url: /mlsys/networking/2025/07/01/scale-up-ethernet-framework-scale-up-ethernet-framework-specification.html
    - categories:
      - mlsys
      - networking
      date: '2025-04-01'
      title: Introducing UALink 200G 1.0 Specification
      url: /mlsys/networking/2025/04/01/introducing-ualink-200g-10-specification.html
    - categories:
      - mlsys
      - networking
      date: '2025-03-01'
      title: 'UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture'
      url: /mlsys/networking/2025/03/01/ub-mesh-a-hierarchically-localized-nd-fullmesh-datacenter-network-architecture.html
    - categories:
      - mlsys
      - networking
      date: '2024-04-01'
      title: Scaling Up Memory Disaggregated Applications with Smart
      url: /mlsys/networking/2024/04/01/scaling-up-memory-disaggregated-applications-with-smart.html
    - categories:
      - mlsys
      - networking
      date: '2023-07-01'
      title: Overview of and Motivation for the Forthcoming Ultra Ethernet Consortium
        Specification
      url: /mlsys/networking/2023/07/01/overview-of-and-motivation-for-the-forthcoming-ultra-ethernet-consortium-specification.html
    - categories:
      - mlsys
      - networking
      date: '2022-11-01'
      title: Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM
        and GPUDirect Async
      url: /mlsys/networking/2022/11/01/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async.html
    - categories:
      - mlsys
      - networking
      date: '2022-02-01'
      title: Doubling all2all Performance with NVIDIA Collective Communication Library
        2.12
      url: /mlsys/networking/2022/02/01/doubling-all2all-performance-with-nvidia-collective-communication-library-212.html
    - categories:
      - mlsys
      - networking
      date: '2020-08-01'
      title: An In-Depth Analysis of the Slingshot Interconnect
      url: /mlsys/networking/2020/08/01/an-in-depth-analysis-of-the-slingshot-interconnect.html
    - categories:
      - mlsys
      - networking
      date: '2015-07-01'
      title: 'UCX: An Open Source Framework for HPC Network APIs and Beyond'
      url: /mlsys/networking/2015/07/01/ucx-an-open-source-framework-for-hpc-network-apis-and-beyond.html
