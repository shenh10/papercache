<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ° | PaperCacheâ€</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ°" />
<meta name="author" content="shenh10" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="LLM é©±åŠ¨çš„ Paper ç¬”è®°ã€‚" />
<meta property="og:description" content="LLM é©±åŠ¨çš„ Paper ç¬”è®°ã€‚" />
<link rel="canonical" href="https://shenh10.github.io/papercache/index.markdown.bak2" />
<meta property="og:url" content="https://shenh10.github.io/papercache/index.markdown.bak2" />
<meta property="og:site_name" content="PaperCacheâ€" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ°" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"shenh10"},"description":"LLM é©±åŠ¨çš„ Paper ç¬”è®°ã€‚","headline":"PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ°","url":"https://shenh10.github.io/papercache/index.markdown.bak2"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/papercache/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://shenh10.github.io/papercache/feed.xml" title="PaperCache&quot;" /><!-- MathJax æ•°å­¦å…¬å¼æ”¯æŒ -->
  

  <!-- ç»Ÿè®¡ä»£ç  -->
  <!-- Google Analytics 4 -->

<script async src="https://www.googletagmanager.com/gtag/js?id=REPLACE_WITH_GA_ID"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'REPLACE_WITH_GA_ID');
</script>


<!-- Plausible Analytics -->


<!-- Umami Analytics -->


</head>

<body>
  <header class="site-header">
    <div class="wrapper">
      <a class="site-title" rel="author" href="/papercache/">PaperCache&quot;</a>
      
      <nav class="site-nav">
        <a href="/papercache/">é¦–é¡µ</a>
        <a href="/papercache/about/">å…³äº</a>
        <a href="https://github.com/shenh10/deepnotes" target="_blank">æºä»£ç </a>
      </nav>
    </div>
  </header>

  <main class="page-content" aria-label="Content">
    <div class="wrapper">
      <article class="post">

  <header class="post-header">
    <h1 class="post-title">PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ°</h1>
  </header>

  <div class="post-content">
    æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ï¼è¿™æ˜¯ä¸€ä¸ªæˆ‘çªå¦‚å…¶æ¥çš„æƒ³æ³•â€”â€”æ—¥å¸¸å·¥ä½œä¸­è¯»paperè¿™ä»¶äº‹çŠ¹å¦‚å®¶å¸¸ä¾¿é¥­éœ€è¦ï¼Œä½†æ˜¯ç¢äºç¬¬äºŒè¯­è¨€éšœç¢æ€»æ˜¯æ— æ³•åšåˆ°é«˜æ•ˆçš„ä¿¡æ¯è¾“å…¥ã€‚ä½†åœ¨AGIæ—¶ä»£ï¼Œç»§ç»­æŒ‰è€åŠæ³•å•ƒæ–‡ç« ã€è®°ç¬”è®°æ˜¯ä¸€ä»¶ä¸å¤Ÿé«˜æ•ˆçš„äº‹æƒ…ï¼Œæˆ‘å¸Œæœ›æœ‰åŠæ³•å¯ä»¥æé«˜çŸ¥è¯†ç¤¾åŒºçš„æ•ˆç‡ã€‚æ®æˆ‘æ‰€çŸ¥ï¼Œ![Paper Cool](https://papers.cool/) æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¿«é€Ÿåˆ·æ–°è®ºæ–‡çš„å·¥å…·ï¼Œä½†ç²¾åº¦ä¸€ç¯‡æ–‡ç« æˆ‘è®¤ä¸ºå½“å‰çš„å·¥å…·è¿˜æ— æ³•æ»¡è¶³æˆ‘çš„éœ€æ±‚ã€‚

äºæ˜¯ä»prompt engineeringå¼€å§‹ï¼Œæˆ‘å®ç°äº†ä¸€å¥—å¯ä»¥è‡ªåŠ¨ç”Ÿæˆè®ºæ–‡ç²¾è¯»åšå®¢çš„å·¥å…·ã€‚å·¥å…·æ‰“ç£¨äº†å¾ˆå¤šéï¼Œåšçš„ä¸å¤Ÿå®Œç¾ï¼Œæˆ–å¤šæˆ–å°‘éœ€è¦ä¸€ç‚¹äººå·¥çš„å‚ä¸â€”â€”äºæ˜¯ä¸ºäº†è®©è¿™ä¸ªè¿‡ç¨‹æ›´æœ‰æ„ä¹‰ï¼Œæƒ åŠæ›´å¤šçš„ç›¸å…³ä»ä¸šè€…åŠå…¥é—¨æ–°äººï¼Œæˆ‘å°†å®ƒå˜æˆäº†è¿™ä¸ªå…¬å¼€çš„åšå®¢ï¼Œä½¿å¾—çŸ¥è¯†ä¸ä»…å¯ä»¥ç§¯ç´¯ã€ä¹Ÿå¯ä»¥ä¼ æ’­ã€æŸ¥è¯¢ä»¥åŠè®¨è®ºã€‚

æˆ‘å¸Œæœ›é€šè¿‡é˜…è¯»è¿™é‡Œçš„ paper åšå®¢ï¼Œå¤§å®¶å¯ä»¥è½»æ¾å¸å–åŸæ–‡80%çš„å†…å®¹ï¼Œè€Œä¸ç”¨å†ä¸æ–­å°†paperåŠ å…¥checklist è®©listè¶Šæ”’è¶Šå¤šã€‚AI ç”Ÿæˆæ–‡ç« å†…å®¹å¯èƒ½ä¼šæœ‰é”™è¯¯ï¼Œå¸Œæœ›å¤§å®¶å¯ä»¥åœ¨è¯„è®ºåŒºç§¯ææŒ‡å‡ºã€‚

æœ€åï¼Œè¿™ä¸ªçŸ¥è¯†åº“ä¼šé•¿æœŸæ›´æ–°ï¼Œä¸»è¦æ”¶è—å…³äº AI Infra å…¨æ ˆç›¸å…³çš„çŸ¥è¯†å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äº **Machine Learning System**ã€**å¤§æ¨¡å‹ç®—æ³•**, **AIåŠ é€Ÿå™¨** ç­‰ç­‰ï¼Œstay tuned!

æœ€åï¼Œæ„Ÿè°¢ Gemini ä¸ Grokï¼Œè®©è¿™ä¸€åˆ‡å˜æˆäº†å¯èƒ½ã€‚

## ğŸ“š è®ºæ–‡åˆ—è¡¨

### æ¨ç†ä¼˜åŒ–
- **[2025-07] [Step 3: Large yet Affordable Model System Co-design for Cost-effective Decoding](/llm/engineering/inference/2025-07-Step-3-is-Large-yet-Affordable-Model-system-Co-design-for-Cost-effective-Decoding.html)** - æˆæœ¬æ•ˆç›Šæ¨¡å‹è§£ç çš„ç³»ç»ŸååŒè®¾è®¡
- **[2025-07] [MegaScale: Infer Serving Mixture of Experts at Scale with Disaggregated Expert Parallelism](/llm/engineering/inference/2025-07-MegaScale-Infer-Serving-Mixture-of-Experts-at-Scale-with-Disaggregated-Expert-Parallelism.html)** - å¤§è§„æ¨¡ä¸“å®¶æ··åˆæ¨¡å‹çš„æ¨ç†æœåŠ¡
- **[2025-05] [Insights into DeepSeek V3: Scaling Challenges and Reflections on Hardware for AI Architecture](/llm/engineering/inference/2025-05-Insights-into-DeepSeek-V3-Scaling-Challenges-and-Reflections-on-Hardware-for-AI-Architecture.html)** - DeepSeek V3çš„æ‰©å±•æŒ‘æˆ˜ä¸AIæ¶æ„ç¡¬ä»¶æ€è€ƒ
- **[2025-04] [Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving](/llm/engineering/inference/2025-04-Tilus-A-Virtual-Machine-for-Arbitrary-Low-Precision-GPGPU-Computation-in-LLM-Serving.html)** - LLMæœåŠ¡ä¸­ä»»æ„ä½ç²¾åº¦GPGPUè®¡ç®—çš„è™šæ‹Ÿæœº
- **[2025-04] [CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving](/llm/engineering/inference/2025-04-CacheGen-KV-Cache-Compression-and-Streaming-for-Fast-Large-Language-Model-Serving.html)** - KVç¼“å­˜å‹ç¼©å’Œæµå¼ä¼ è¾“çš„å¿«é€Ÿå¤§è¯­è¨€æ¨¡å‹æœåŠ¡
- **[2025-04] [CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion](/llm/engineering/inference/2025-04-CacheBlend-Fast-Large-Language-Model-Serving-for-RAG-with-Cached-Knowledge-Fusion.html)** - åŸºäºç¼“å­˜çŸ¥è¯†èåˆçš„å¿«é€Ÿå¤§è¯­è¨€æ¨¡å‹RAGæœåŠ¡
- **[2024-10] [Do Large Language Models Need a Content Delivery Network?](/llm/engineering/inference/2024-10-Do-Large-Language-Models-Need-a-Content-Delivery-Network.html)** - å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦éœ€è¦å†…å®¹åˆ†å‘ç½‘ç»œï¼Ÿ
- **[2023-09] [Efficient Memory Management for Large Language Model Serving with PagedAttention](/llm/engineering/inference/2023-09-Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention.html)** - åŸºäºåˆ†é¡µæ³¨æ„åŠ›çš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å†…å­˜ç®¡ç†
- **[2023-08] [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](/llm/engineering/inference/2023-08-SARATHI-Efficient-LLM-Inference-by-Piggybacking-Decodes-with-Chunked-Prefills.html)** - é€šè¿‡åˆ†å—é¢„å¡«å……æå¸¦è§£ç çš„é«˜æ•ˆLLMæ¨ç†
## ğŸ“š è®ºæ–‡åˆ—è¡¨


### æ¨ç†ä¼˜åŒ–

- [2025-07-30] [Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding](/llm/engineering/inference//2025/07/30/step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding.html)
- [2025-07-30] [MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism](/llm/engineering/inference//2025/07/30/megascale-infer-serving-mixture-of-experts-at-scale-with-disaggregated-expert-parallelism.html)
- [2025-05-30] [Recipes for Pre-training LLMs with MXFP8](/llm/engineering/inference//2025/05/30/recipes-for-pre-training-llms-with-mxfp8.html)
- [2025-05-30] [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](/llm/engineering/inference//2025/05/30/insights-into-deepseek-v3-scaling-challenges-and-reflections-on-hardware-for-ai-architectures.html)
- [2025-04-30] [Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving](/llm/engineering/inference//2025/04/30/tilus-a-virtual-machine-for-arbitrary-low-precision-gpgpu-computation-in-llm-serving.html)
- [2025-03-30] [EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test](/llm/engineering/inference//2025/03/30/eagle-3-scaling-up-inference-acceleration-of-large-language-models-via-training-time-test.html)
- [2024-10-30] [Do Large Language Models Need a Content Delivery Network?](/llm/engineering/inference//2024/10/30/do-large-language-models-need-a-content-delivery-network.html)
- [2024-08-30] [NanoFlow: Towards Optimal Large Language Model Serving Throughput](/llm/engineering/inference//2024/08/30/nanoflow-towards-optimal-large-language-model-serving-throughput.html)
- [2024-07-30] [SGLang: Efficient Execution of Structured Language Model Programs](/llm/engineering/inference//2024/07/30/sglang-efficient-execution-of-structured-language-model-programs.html)
- [2024-07-30] [Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving](/llm/engineering/inference//2024/07/30/mooncake-a-kvcache-centric-disaggregated-architecture-for-llm-serving.html)
- [2024-07-30] [Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention](/llm/engineering/inference//2024/07/30/cost-efficient-large-language-model-serving-for-multi-turn-conversations-with-cachedattention.html)
- [2024-06-30] [MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](/llm/engineering/inference//2024/06/30/medusa-simple-llm-inference-acceleration-framework-with-multiple-decoding-heads.html)
- [2024-06-30] [EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees](/llm/engineering/inference//2024/06/30/eagle-2-faster-inference-of-language-models-with-dynamic-draft-trees.html)
- [2024-05-30] [Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation](/llm/engineering/inference//2024/05/30/efficient-heterogeneous-large-language-model-decoding-with-model-attention-disaggregation.html)
- [2024-05-30] [CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion](/llm/engineering/inference//2024/05/30/cacheblend-fast-large-language-model-serving-for-rag-with-cached-knowledge-fusion.html)
- [2024-04-30] [PROMPT CACHE: MODULAR ATTENTION REUSE FOR LOW-LATENCY INFERENCE](/llm/engineering/inference//2024/04/30/prompt-cache-modular-attention-reuse-for-low-latency-inference.html)
- [2024-01-30] [FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design](/llm/engineering/inference//2024/01/30/fp6-llm-efficiently-serving-large-language-models-through-fp6-centric-algorithm-system-co-design.html)
- [2024-01-30] [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](/llm/engineering/inference//2024/01/30/eagle-speculative-sampling-requires-rethinking-feature-uncertainty.html)
- [2024-01-30] [DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving](/llm/engineering/inference//2024/01/30/distserve-disaggregating-prefill-and-decoding-for-goodput-optimized-large-language-model-serving.html)
- [2023-10-30] [CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving](/llm/engineering/inference//2023/10/30/cachegen-kv-cache-compression-and-streaming-for-fast-large-language-model-serving.html)
- [2023-09-30] [Efficient Memory Management for Large Language Model Serving with PagedAttention](/llm/engineering/inference//2023/09/30/efficient-memory-management-for-large-language-model-serving-with-pagedattention.html)
- [2023-08-30] [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](/llm/engineering/inference//2023/08/30/sarathi-efficient-llm-inference-by-piggybacking-decodes-with-chunked-prefills.html)
- [2023-06-30] [FP8 versus INT8 for efficient deep learning inference](/llm/engineering/inference//2023/06/30/fp8-versus-int8-for-efficient-deep-learning-inference.html)
- [2023-06-30] [AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION FOR ON-DEVICE LLM COMPRESSION AND ACCELERATION](/llm/engineering/inference//2023/06/30/awq-activation-aware-weight-quantization-for-on-device-llm-compression-and-acceleration.html)
- [2023-05-30] [Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models](/llm/engineering/inference//2023/05/30/integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models.html)
- [2023-04-30] [With Shared Microexponents, A Little Shifting Goes a Long Way](/llm/engineering/inference//2023/04/30/with-shared-microexponents-a-little-shifting-goes-a-long-way.html)
- [2022-11-30] [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](/llm/engineering/inference//2022/11/30/smoothquant-accurate-and-efficient-post-training-quantization-for-large-language-models.html)
- [2022-07-30] [Orca: A Distributed Serving System for Transformer-Based Generative Models](/llm/engineering/inference//2022/07/30/orca-a-distributed-serving-system-for-transformer-based-generative-models.html)

### è®­ç»ƒä¼˜åŒ–


### é¢„è®­ç»ƒæ–¹æ³•

- [2022-03-30] [](/llm/algorithm/pretrain//2022/03/30/tensor-programs-v-tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer.html)
- [2021-04-30] [ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING](/llm/algorithm/pretrain//2021/04/30/roformer-enhanced-transformer-with-rotary-position-embedding.html)

### å¼ºåŒ–å­¦ä¹ 


### GPUåŠ é€Ÿä¸ç¼–è¯‘å™¨ä¼˜åŒ–

- [2025-05-30] [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training](/llm/engineering/attention//2025/05/30/sageattention3-microscaling-fp4-attention-for-inference-and-an-exploration-of-8-bit-training.html)
- [2025-02-28] [TREE ATTENTION: TOPOLOGY-AWARE DECODING FOR LONG-CONTEXT ATTENTION ON GPU CLUSTERS](/llm/engineering/attention//2025/02/28/tree-attention-topology-aware-decoding-for-long-context-attention-on-gpu-clusters.html)
- [2025-02-28] [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](/llm/engineering/attention//2025/02/28/native-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention.html)
- [2025-02-28] [MOBA: MIXTURE OF BLOCK ATTENTION FOR LONG-CONTEXT LLMS](/llm/engineering/attention//2025/02/28/moba-mixture-of-block-attention-for-long-context-llms.html)
- [2025-01-30] [MiniMax-01: Scaling Foundation Models with Lightning Attention](/llm/engineering/attention//2025/01/30/minimax-01-scaling-foundation-models-with-lightning-attention.html)
- [2024-10-30] [SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION](/llm/engineering/attention//2024/10/30/sageattention-accurate-8-bit-attention-for-plug-and-play-inference-acceleration.html)
- [2024-07-30] [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](/llm/engineering/attention//2024/07/30/flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision.html)
- [2024-04-30] [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](/llm/engineering/attention//2024/04/30/leave-no-context-behind-efficient-infinite-context-transformers-with-infini-attention.html)
- [2023-12-30] [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](/llm/engineering/attention//2023/12/30/gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints.html)
- [2023-07-30] [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](/llm/engineering/attention//2023/07/30/flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning.html)
- [2022-07-30] [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](/llm/engineering/attention//2022/07/30/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness.html)
- [2020-06-30] [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](/llm/engineering/attention//2020/06/30/transformers-are-rnns-fast-autoregressive-transformers-with-linear-attention.html)
- [2020-02-28] [Low-Rank Bottleneck in Multi-head Attention Models](/llm/engineering/attention//2020/02/28/low-rank-bottleneck-in-multi-head-attention-models.html)
- [2019-11-30] [Fast Transformer Decoding: One Write-Head is All You Need](/llm/engineering/attention//2019/11/30/fast-transformer-decoding-one-write-head-is-all-you-need.html)

### ç¼–è¯‘å™¨ä¸ç¼–ç¨‹æ¨¡å‹

- [2025-06-30] [Mirage: A Multi-Level Superoptimizer for Tensor Programs](/llm/engineering/compiler//2025/06/30/mirage-a-multi-level-superoptimizer-for-tensor-programs.html)
- [2025-04-30] [TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives](/llm/engineering/compiler//2025/04/30/tilelink-generating-efficient-compute-communication-overlapping-kernels-using-tile-centric-primitives.html)
- [2025-04-30] [TileLang: A Composable Tiled Programming Model for AI Systems](/llm/engineering/compiler//2025/04/30/tilelang-a-composable-tiled-programming-model-for-ai-systems.html)
- [2024-12-30] [FLEX ATTENTION: A PROGRAMMING MODEL FOR GENERATING OPTIMIZED ATTENTION KERNELS](/llm/engineering/compiler//2024/12/30/flex-attention-a-programming-model-for-generating-optimized-attention-kernels.html)
- [2024-10-30] [FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL FUSION](/llm/engineering/compiler//2024/10/30/flux-fast-software-based-communication-overlap-on-gpus-through-kernel-fusion.html)

### æœºå™¨å­¦ä¹ ç³»ç»Ÿ

- [2025-07-30] [Scale-Up Ethernet Framework Scale-Up Ethernet Framework Specification](/mlsys//networking/2025/07/30/scale-up-ethernet-framework-scale-up-ethernet-framework-specification.html)
- [2025-07-30] [](/mlsys//networking/2025/07/30/demystifying-nccl-an-in-depth-analysis-of-gpu-communication-protocols-and-algorithms.html)
- [2025-04-30] [Introducing UALink 200G 1.0 Specification](/mlsys//networking/2025/04/30/introducing-ualink-200g-10-specification.html)
- [2025-03-30] [UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture](/mlsys//networking/2025/03/30/ub-mesh-a-hierarchically-localized-nd-fullmesh-datacenter-network-architecture.html)
- [2024-04-30] [Scaling Up Memory Disaggregated Applications with Smart](/mlsys//networking/2024/04/30/scaling-up-memory-disaggregated-applications-with-smart.html)
- [2023-07-30] [Overview of and Motivation for the Forthcoming Ultra Ethernet Consortium Specification](/mlsys//networking/2023/07/30/overview-of-and-motivation-for-the-forthcoming-ultra-ethernet-consortium-specification.html)
- [2022-11-30] [Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM and GPUDirect Async](/mlsys//networking/2022/11/30/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async.html)
- [2022-02-28] [Doubling all2all Performance with NVIDIA Collective Communication Library 2.12](/mlsys//networking/2022/02/28/doubling-all2all-performance-with-nvidia-collective-communication-library-212.html)
- [2020-08-30] [An In-Depth Analysis of the Slingshot Interconnect](/mlsys//networking/2020/08/30/an-in-depth-analysis-of-the-slingshot-interconnect.html)
- [2015-07-30] [UCX: An Open Source Framework for HPC Network APIs and Beyond](/mlsys//networking/2015/07/30/ucx-an-open-source-framework-for-hpc-network-apis-and-beyond.html)
- [2025-08-23] [Demystifying GPU Microarchitecture through Microbenchmarking](/mlsys//gpu/2025/08/23/demystifying-gpu-microarchitecture-through-microbenchmarking.html)
- [2023-01-30] [Stream-K: Work-centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU](/mlsys//gpu/2023/01/30/stream-k-work-centric-parallel-decomposition-for-dense-matrix-matrix-multiplication-on-the-gpu.html)
 - [2020-02-28] [GPU Initiated OpenSHMEM: Correct and Efficient Intra-Kernel Networking for dGPUs](/mlsys//gpu/2020/02/28/gpu-initiated-openshmem-correct-and-eicient-intra-kernel-networking-for-dgpus.html)
- [2018-03-30] [Improving Real-Time Performance with CUDA Persistent Threads (CuPer) on the Jetson TX2](/mlsys//gpu/2018/03/30/improving-real-time-performance-with-cuda-persistent-threads-cuper-on-the-jetson-tx2.html)
- [2017-04-30] [Locality-Aware CTA Clustering for Modern GPUs](/mlsys//gpu/2017/04/30/locality-aware-cta-clustering-for-modern-gpus.html)
- [2016-04-30] [Optimizing Performance of Recurrent Neural Networks on GPUs](/mlsys//gpu/2016/04/30/optimizing-performance-of-recurrent-neural-networks-on-gpus.html)
- [2025-08-23] [Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training](/mlsys//framework/2025/08/23/campo-cost-aware-performance-optimization-for-mixed-precision-neural-network-training.html)
- **[2025-07] [Demystifying NCCL: An In-depth Analysis of GPU Communications Protocols and Algorithms](/mlsys/networking/2025-07-Demystifying-NCCL-An-In-depth-Analysis-of-GPU-Communications-Protocols-and-Algorithms.html)** - æ·±å…¥åˆ†æGPUé€šä¿¡åè®®å’Œç®—æ³•
- **[2015-07] [UCX: An Open Source Framework for HPC Network APIs and Beyond](/mlsys/networking/2015-07-UCX-An-Open-Source-Framework-for-HPC-Network-APIs-and-Beyond.html)** - é«˜æ€§èƒ½è®¡ç®—ç½‘ç»œAPIåŠæ›´å¤šåŠŸèƒ½çš„å¼€æºæ¡†æ¶

---

## å…³äº

**[äº†è§£æ›´å¤š â†’](/about/)**

---

  </div>

</article>

    </div>
  </main>

  <footer class="site-footer">
    <div class="wrapper">
      <div class="footer-content">
        <div class="footer-col-1">
          <h3>PaperCache"</h3>
          <p>æ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½ç ”ç©¶è®ºæ–‡çš„ç²¾é€‰ç¼“å­˜</p>
        </div>
        
        <div class="footer-col-2">
          <h3>é“¾æ¥</h3>
          <ul>
            <li><a href="https://github.com/shenh10/deepnotes">æºä»£ç ä»“åº“</a></li>
            <li><a href="/papercache/feed.xml">RSSè®¢é˜…</a></li>
          </ul>
        </div>
        
        <div class="footer-col-3">
          <!-- è®¿é—®ç»Ÿè®¡ -->
          
          <div class="site-stats">
            <span id="busuanzi_container_site_pv" style="display:none">
              æ€»è®¿é—®é‡ <span id="busuanzi_value_site_pv"></span> æ¬¡
            </span>
            <span id="busuanzi_container_site_uv" style="display:none">
              è®¿å®¢æ•° <span id="busuanzi_value_site_uv"></span> äºº
            </span>
          </div>
          <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
          
        </div>
      </div>
      
      <div class="footer-bottom">
        <p>&copy; 2025 shenh10. 
           ç”± <a href="https://github.com/shenh10/deepnotes">Deep Notes</a> è‡ªåŠ¨ç”Ÿæˆ</p>
      </div>
    </div>
  </footer>

</body>
</html>
