<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>PaperCache - 我的Paper收藏自留地 | PaperCache”</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="PaperCache - 我的Paper收藏自留地" />
<meta name="author" content="shenh10" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="LLM 驱动的 Paper 笔记。" />
<meta property="og:description" content="LLM 驱动的 Paper 笔记。" />
<link rel="canonical" href="https://shenh10.github.io/papercache/index.markdown.bak2" />
<meta property="og:url" content="https://shenh10.github.io/papercache/index.markdown.bak2" />
<meta property="og:site_name" content="PaperCache”" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="PaperCache - 我的Paper收藏自留地" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"shenh10"},"description":"LLM 驱动的 Paper 笔记。","headline":"PaperCache - 我的Paper收藏自留地","url":"https://shenh10.github.io/papercache/index.markdown.bak2"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/papercache/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://shenh10.github.io/papercache/feed.xml" title="PaperCache&quot;" /><!-- MathJax 数学公式支持 -->
  

  <!-- 统计代码 -->
  <!-- Google Analytics 4 -->

<script async src="https://www.googletagmanager.com/gtag/js?id=REPLACE_WITH_GA_ID"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'REPLACE_WITH_GA_ID');
</script>


<!-- Plausible Analytics -->


<!-- Umami Analytics -->


</head>

<body>
  <header class="site-header">
    <div class="wrapper">
      <a class="site-title" rel="author" href="/papercache/">PaperCache&quot;</a>
      
      <nav class="site-nav">
        <a href="/papercache/">首页</a>
        <a href="/papercache/about/">关于</a>
        <a href="https://github.com/shenh10/deepnotes" target="_blank">源代码</a>
      </nav>
    </div>
  </header>

  <main class="page-content" aria-label="Content">
    <div class="wrapper">
      <article class="post">

  <header class="post-header">
    <h1 class="post-title">PaperCache - 我的Paper收藏自留地</h1>
  </header>

  <div class="post-content">
    欢迎来到我的博客！这是一个我突如其来的想法——日常工作中读paper这件事犹如家常便饭需要，但是碍于第二语言障碍总是无法做到高效的信息输入。但在AGI时代，继续按老办法啃文章、记笔记是一件不够高效的事情，我希望有办法可以提高知识社区的效率。据我所知，![Paper Cool](https://papers.cool/) 是一个很好的快速刷新论文的工具，但精度一篇文章我认为当前的工具还无法满足我的需求。

于是从prompt engineering开始，我实现了一套可以自动生成论文精读博客的工具。工具打磨了很多遍，做的不够完美，或多或少需要一点人工的参与——于是为了让这个过程更有意义，惠及更多的相关从业者及入门新人，我将它变成了这个公开的博客，使得知识不仅可以积累、也可以传播、查询以及讨论。

我希望通过阅读这里的 paper 博客，大家可以轻松吸取原文80%的内容，而不用再不断将paper加入checklist 让list越攒越多。AI 生成文章内容可能会有错误，希望大家可以在评论区积极指出。

最后，这个知识库会长期更新，主要收藏关于 AI Infra 全栈相关的知识内容，包括但不限于 **Machine Learning System**、**大模型算法**, **AI加速器** 等等，stay tuned!

最后，感谢 Gemini 与 Grok，让这一切变成了可能。

## 📚 论文列表

### 推理优化
- **[2025-07] [Step 3: Large yet Affordable Model System Co-design for Cost-effective Decoding](/llm/engineering/inference/2025-07-Step-3-is-Large-yet-Affordable-Model-system-Co-design-for-Cost-effective-Decoding.html)** - 成本效益模型解码的系统协同设计
- **[2025-07] [MegaScale: Infer Serving Mixture of Experts at Scale with Disaggregated Expert Parallelism](/llm/engineering/inference/2025-07-MegaScale-Infer-Serving-Mixture-of-Experts-at-Scale-with-Disaggregated-Expert-Parallelism.html)** - 大规模专家混合模型的推理服务
- **[2025-05] [Insights into DeepSeek V3: Scaling Challenges and Reflections on Hardware for AI Architecture](/llm/engineering/inference/2025-05-Insights-into-DeepSeek-V3-Scaling-Challenges-and-Reflections-on-Hardware-for-AI-Architecture.html)** - DeepSeek V3的扩展挑战与AI架构硬件思考
- **[2025-04] [Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving](/llm/engineering/inference/2025-04-Tilus-A-Virtual-Machine-for-Arbitrary-Low-Precision-GPGPU-Computation-in-LLM-Serving.html)** - LLM服务中任意低精度GPGPU计算的虚拟机
- **[2025-04] [CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving](/llm/engineering/inference/2025-04-CacheGen-KV-Cache-Compression-and-Streaming-for-Fast-Large-Language-Model-Serving.html)** - KV缓存压缩和流式传输的快速大语言模型服务
- **[2025-04] [CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion](/llm/engineering/inference/2025-04-CacheBlend-Fast-Large-Language-Model-Serving-for-RAG-with-Cached-Knowledge-Fusion.html)** - 基于缓存知识融合的快速大语言模型RAG服务
- **[2024-10] [Do Large Language Models Need a Content Delivery Network?](/llm/engineering/inference/2024-10-Do-Large-Language-Models-Need-a-Content-Delivery-Network.html)** - 大语言模型是否需要内容分发网络？
- **[2023-09] [Efficient Memory Management for Large Language Model Serving with PagedAttention](/llm/engineering/inference/2023-09-Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention.html)** - 基于分页注意力的大语言模型服务内存管理
- **[2023-08] [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](/llm/engineering/inference/2023-08-SARATHI-Efficient-LLM-Inference-by-Piggybacking-Decodes-with-Chunked-Prefills.html)** - 通过分块预填充捎带解码的高效LLM推理
## 📚 论文列表


### 推理优化

- [2025-07-30] [Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding](/llm/engineering/inference//2025/07/30/step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding.html)
- [2025-07-30] [MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism](/llm/engineering/inference//2025/07/30/megascale-infer-serving-mixture-of-experts-at-scale-with-disaggregated-expert-parallelism.html)
- [2025-05-30] [Recipes for Pre-training LLMs with MXFP8](/llm/engineering/inference//2025/05/30/recipes-for-pre-training-llms-with-mxfp8.html)
- [2025-05-30] [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](/llm/engineering/inference//2025/05/30/insights-into-deepseek-v3-scaling-challenges-and-reflections-on-hardware-for-ai-architectures.html)
- [2025-04-30] [Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving](/llm/engineering/inference//2025/04/30/tilus-a-virtual-machine-for-arbitrary-low-precision-gpgpu-computation-in-llm-serving.html)
- [2025-03-30] [EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test](/llm/engineering/inference//2025/03/30/eagle-3-scaling-up-inference-acceleration-of-large-language-models-via-training-time-test.html)
- [2024-10-30] [Do Large Language Models Need a Content Delivery Network?](/llm/engineering/inference//2024/10/30/do-large-language-models-need-a-content-delivery-network.html)
- [2024-08-30] [NanoFlow: Towards Optimal Large Language Model Serving Throughput](/llm/engineering/inference//2024/08/30/nanoflow-towards-optimal-large-language-model-serving-throughput.html)
- [2024-07-30] [SGLang: Efficient Execution of Structured Language Model Programs](/llm/engineering/inference//2024/07/30/sglang-efficient-execution-of-structured-language-model-programs.html)
- [2024-07-30] [Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving](/llm/engineering/inference//2024/07/30/mooncake-a-kvcache-centric-disaggregated-architecture-for-llm-serving.html)
- [2024-07-30] [Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention](/llm/engineering/inference//2024/07/30/cost-efficient-large-language-model-serving-for-multi-turn-conversations-with-cachedattention.html)
- [2024-06-30] [MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](/llm/engineering/inference//2024/06/30/medusa-simple-llm-inference-acceleration-framework-with-multiple-decoding-heads.html)
- [2024-06-30] [EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees](/llm/engineering/inference//2024/06/30/eagle-2-faster-inference-of-language-models-with-dynamic-draft-trees.html)
- [2024-05-30] [Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation](/llm/engineering/inference//2024/05/30/efficient-heterogeneous-large-language-model-decoding-with-model-attention-disaggregation.html)
- [2024-05-30] [CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion](/llm/engineering/inference//2024/05/30/cacheblend-fast-large-language-model-serving-for-rag-with-cached-knowledge-fusion.html)
- [2024-04-30] [PROMPT CACHE: MODULAR ATTENTION REUSE FOR LOW-LATENCY INFERENCE](/llm/engineering/inference//2024/04/30/prompt-cache-modular-attention-reuse-for-low-latency-inference.html)
- [2024-01-30] [FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design](/llm/engineering/inference//2024/01/30/fp6-llm-efficiently-serving-large-language-models-through-fp6-centric-algorithm-system-co-design.html)
- [2024-01-30] [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](/llm/engineering/inference//2024/01/30/eagle-speculative-sampling-requires-rethinking-feature-uncertainty.html)
- [2024-01-30] [DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving](/llm/engineering/inference//2024/01/30/distserve-disaggregating-prefill-and-decoding-for-goodput-optimized-large-language-model-serving.html)
- [2023-10-30] [CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving](/llm/engineering/inference//2023/10/30/cachegen-kv-cache-compression-and-streaming-for-fast-large-language-model-serving.html)
- [2023-09-30] [Efficient Memory Management for Large Language Model Serving with PagedAttention](/llm/engineering/inference//2023/09/30/efficient-memory-management-for-large-language-model-serving-with-pagedattention.html)
- [2023-08-30] [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](/llm/engineering/inference//2023/08/30/sarathi-efficient-llm-inference-by-piggybacking-decodes-with-chunked-prefills.html)
- [2023-06-30] [FP8 versus INT8 for efficient deep learning inference](/llm/engineering/inference//2023/06/30/fp8-versus-int8-for-efficient-deep-learning-inference.html)
- [2023-06-30] [AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION FOR ON-DEVICE LLM COMPRESSION AND ACCELERATION](/llm/engineering/inference//2023/06/30/awq-activation-aware-weight-quantization-for-on-device-llm-compression-and-acceleration.html)
- [2023-05-30] [Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models](/llm/engineering/inference//2023/05/30/integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models.html)
- [2023-04-30] [With Shared Microexponents, A Little Shifting Goes a Long Way](/llm/engineering/inference//2023/04/30/with-shared-microexponents-a-little-shifting-goes-a-long-way.html)
- [2022-11-30] [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](/llm/engineering/inference//2022/11/30/smoothquant-accurate-and-efficient-post-training-quantization-for-large-language-models.html)
- [2022-07-30] [Orca: A Distributed Serving System for Transformer-Based Generative Models](/llm/engineering/inference//2022/07/30/orca-a-distributed-serving-system-for-transformer-based-generative-models.html)

### 训练优化


### 预训练方法

- [2022-03-30] [](/llm/algorithm/pretrain//2022/03/30/tensor-programs-v-tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer.html)
- [2021-04-30] [ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING](/llm/algorithm/pretrain//2021/04/30/roformer-enhanced-transformer-with-rotary-position-embedding.html)

### 强化学习


### GPU加速与编译器优化

- [2025-05-30] [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training](/llm/engineering/attention//2025/05/30/sageattention3-microscaling-fp4-attention-for-inference-and-an-exploration-of-8-bit-training.html)
- [2025-02-28] [TREE ATTENTION: TOPOLOGY-AWARE DECODING FOR LONG-CONTEXT ATTENTION ON GPU CLUSTERS](/llm/engineering/attention//2025/02/28/tree-attention-topology-aware-decoding-for-long-context-attention-on-gpu-clusters.html)
- [2025-02-28] [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](/llm/engineering/attention//2025/02/28/native-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention.html)
- [2025-02-28] [MOBA: MIXTURE OF BLOCK ATTENTION FOR LONG-CONTEXT LLMS](/llm/engineering/attention//2025/02/28/moba-mixture-of-block-attention-for-long-context-llms.html)
- [2025-01-30] [MiniMax-01: Scaling Foundation Models with Lightning Attention](/llm/engineering/attention//2025/01/30/minimax-01-scaling-foundation-models-with-lightning-attention.html)
- [2024-10-30] [SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION](/llm/engineering/attention//2024/10/30/sageattention-accurate-8-bit-attention-for-plug-and-play-inference-acceleration.html)
- [2024-07-30] [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](/llm/engineering/attention//2024/07/30/flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision.html)
- [2024-04-30] [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](/llm/engineering/attention//2024/04/30/leave-no-context-behind-efficient-infinite-context-transformers-with-infini-attention.html)
- [2023-12-30] [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](/llm/engineering/attention//2023/12/30/gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints.html)
- [2023-07-30] [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](/llm/engineering/attention//2023/07/30/flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning.html)
- [2022-07-30] [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](/llm/engineering/attention//2022/07/30/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness.html)
- [2020-06-30] [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](/llm/engineering/attention//2020/06/30/transformers-are-rnns-fast-autoregressive-transformers-with-linear-attention.html)
- [2020-02-28] [Low-Rank Bottleneck in Multi-head Attention Models](/llm/engineering/attention//2020/02/28/low-rank-bottleneck-in-multi-head-attention-models.html)
- [2019-11-30] [Fast Transformer Decoding: One Write-Head is All You Need](/llm/engineering/attention//2019/11/30/fast-transformer-decoding-one-write-head-is-all-you-need.html)

### 编译器与编程模型

- [2025-06-30] [Mirage: A Multi-Level Superoptimizer for Tensor Programs](/llm/engineering/compiler//2025/06/30/mirage-a-multi-level-superoptimizer-for-tensor-programs.html)
- [2025-04-30] [TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives](/llm/engineering/compiler//2025/04/30/tilelink-generating-efficient-compute-communication-overlapping-kernels-using-tile-centric-primitives.html)
- [2025-04-30] [TileLang: A Composable Tiled Programming Model for AI Systems](/llm/engineering/compiler//2025/04/30/tilelang-a-composable-tiled-programming-model-for-ai-systems.html)
- [2024-12-30] [FLEX ATTENTION: A PROGRAMMING MODEL FOR GENERATING OPTIMIZED ATTENTION KERNELS](/llm/engineering/compiler//2024/12/30/flex-attention-a-programming-model-for-generating-optimized-attention-kernels.html)
- [2024-10-30] [FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL FUSION](/llm/engineering/compiler//2024/10/30/flux-fast-software-based-communication-overlap-on-gpus-through-kernel-fusion.html)

### 机器学习系统

- [2025-07-30] [Scale-Up Ethernet Framework Scale-Up Ethernet Framework Specification](/mlsys//networking/2025/07/30/scale-up-ethernet-framework-scale-up-ethernet-framework-specification.html)
- [2025-07-30] [](/mlsys//networking/2025/07/30/demystifying-nccl-an-in-depth-analysis-of-gpu-communication-protocols-and-algorithms.html)
- [2025-04-30] [Introducing UALink 200G 1.0 Specification](/mlsys//networking/2025/04/30/introducing-ualink-200g-10-specification.html)
- [2025-03-30] [UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture](/mlsys//networking/2025/03/30/ub-mesh-a-hierarchically-localized-nd-fullmesh-datacenter-network-architecture.html)
- [2024-04-30] [Scaling Up Memory Disaggregated Applications with Smart](/mlsys//networking/2024/04/30/scaling-up-memory-disaggregated-applications-with-smart.html)
- [2023-07-30] [Overview of and Motivation for the Forthcoming Ultra Ethernet Consortium Specification](/mlsys//networking/2023/07/30/overview-of-and-motivation-for-the-forthcoming-ultra-ethernet-consortium-specification.html)
- [2022-11-30] [Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM and GPUDirect Async](/mlsys//networking/2022/11/30/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async.html)
- [2022-02-28] [Doubling all2all Performance with NVIDIA Collective Communication Library 2.12](/mlsys//networking/2022/02/28/doubling-all2all-performance-with-nvidia-collective-communication-library-212.html)
- [2020-08-30] [An In-Depth Analysis of the Slingshot Interconnect](/mlsys//networking/2020/08/30/an-in-depth-analysis-of-the-slingshot-interconnect.html)
- [2015-07-30] [UCX: An Open Source Framework for HPC Network APIs and Beyond](/mlsys//networking/2015/07/30/ucx-an-open-source-framework-for-hpc-network-apis-and-beyond.html)
- [2025-08-23] [Demystifying GPU Microarchitecture through Microbenchmarking](/mlsys//gpu/2025/08/23/demystifying-gpu-microarchitecture-through-microbenchmarking.html)
- [2023-01-30] [Stream-K: Work-centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU](/mlsys//gpu/2023/01/30/stream-k-work-centric-parallel-decomposition-for-dense-matrix-matrix-multiplication-on-the-gpu.html)
 - [2020-02-28] [GPU Initiated OpenSHMEM: Correct and Efficient Intra-Kernel Networking for dGPUs](/mlsys//gpu/2020/02/28/gpu-initiated-openshmem-correct-and-eicient-intra-kernel-networking-for-dgpus.html)
- [2018-03-30] [Improving Real-Time Performance with CUDA Persistent Threads (CuPer) on the Jetson TX2](/mlsys//gpu/2018/03/30/improving-real-time-performance-with-cuda-persistent-threads-cuper-on-the-jetson-tx2.html)
- [2017-04-30] [Locality-Aware CTA Clustering for Modern GPUs](/mlsys//gpu/2017/04/30/locality-aware-cta-clustering-for-modern-gpus.html)
- [2016-04-30] [Optimizing Performance of Recurrent Neural Networks on GPUs](/mlsys//gpu/2016/04/30/optimizing-performance-of-recurrent-neural-networks-on-gpus.html)
- [2025-08-23] [Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training](/mlsys//framework/2025/08/23/campo-cost-aware-performance-optimization-for-mixed-precision-neural-network-training.html)
- **[2025-07] [Demystifying NCCL: An In-depth Analysis of GPU Communications Protocols and Algorithms](/mlsys/networking/2025-07-Demystifying-NCCL-An-In-depth-Analysis-of-GPU-Communications-Protocols-and-Algorithms.html)** - 深入分析GPU通信协议和算法
- **[2015-07] [UCX: An Open Source Framework for HPC Network APIs and Beyond](/mlsys/networking/2015-07-UCX-An-Open-Source-Framework-for-HPC-Network-APIs-and-Beyond.html)** - 高性能计算网络API及更多功能的开源框架

---

## 关于

**[了解更多 →](/about/)**

---

  </div>

</article>

    </div>
  </main>

  <footer class="site-footer">
    <div class="wrapper">
      <div class="footer-content">
        <div class="footer-col-1">
          <h3>PaperCache"</h3>
          <p>深度学习、机器学习和人工智能研究论文的精选缓存</p>
        </div>
        
        <div class="footer-col-2">
          <h3>链接</h3>
          <ul>
            <li><a href="https://github.com/shenh10/deepnotes">源代码仓库</a></li>
            <li><a href="/papercache/feed.xml">RSS订阅</a></li>
          </ul>
        </div>
        
        <div class="footer-col-3">
          <!-- 访问统计 -->
          
          <div class="site-stats">
            <span id="busuanzi_container_site_pv" style="display:none">
              总访问量 <span id="busuanzi_value_site_pv"></span> 次
            </span>
            <span id="busuanzi_container_site_uv" style="display:none">
              访客数 <span id="busuanzi_value_site_uv"></span> 人
            </span>
          </div>
          <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
          
        </div>
      </div>
      
      <div class="footer-bottom">
        <p>&copy; 2025 shenh10. 
           由 <a href="https://github.com/shenh10/deepnotes">Deep Notes</a> 自动生成</p>
      </div>
    </div>
  </footer>

</body>
</html>
