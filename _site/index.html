<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>🤖 欢迎来到 PaperCache！ | PaperCache</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="🤖 欢迎来到 PaperCache！" />
<meta name="author" content="shenh10" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="深度学习、机器学习和人工智能研究论文的精选缓存" />
<meta property="og:description" content="深度学习、机器学习和人工智能研究论文的精选缓存" />
<link rel="canonical" href="http://localhost:4000/papercache/" />
<meta property="og:url" content="http://localhost:4000/papercache/" />
<meta property="og:site_name" content="PaperCache" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="🤖 欢迎来到 PaperCache！" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"shenh10"},"description":"深度学习、机器学习和人工智能研究论文的精选缓存","headline":"🤖 欢迎来到 PaperCache！","name":"PaperCache","url":"http://localhost:4000/papercache/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/papercache/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/papercache/feed.xml" title="PaperCache" /><!-- MathJax 3 Configuration -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\(', '\)']],
        displayMath: [['$$', '$$'], ['\[', '\]']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <!-- Busuanzi Analytics -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/papercache/">PaperCache</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/papercache/about/">关于</a><a class="page-link" href="/papercache/collection.html">论文合集</a><a class="page-link" href="/papercache/">🤖 欢迎来到 PaperCache！</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">🤖 欢迎来到 PaperCache！</h1>
  </header>

  <div class="post-content">
    <p><strong>一个由 AI 当实习生，我做老板的博客。</strong></p>

<p>没错，这里的每一篇论文精读，都不是我亲手写的，而是我的赛博雇员——<strong>LLM</strong>——的作品。我只负责投喂论文，并偶尔在它出错时进行一些人工干预。</p>

<hr />

<h3 id="关于这个博客的诞生">关于这个博客的诞生</h3>

<p>作为一个天天被 Paper 追着跑的工程师，我曾挣扎在第二语言和读不完的文献列表里。在这个 AGI 时代，我意识到一个简单有效的方法：<strong>让 AI 去读那些关于 AI 的论文。</strong></p>

<p>于是，PaperCache 诞生了。它是我通过 Prompt Engineering 调教出的一套自动化工具的产物。</p>

<p>我们的目标很明确：</p>
<ul>
  <li>帮你过滤掉论文中 80% 的复杂噪音，让你迅速吸收核心思想。</li>
  <li>把你长长的 “稍后读” 清单，变成高效的 “已读” 列表。</li>
</ul>

<p>当然，我的 AI 伙伴有时会过于自信，可能会犯一些错误。如果你发现了任何 Bug，<strong>请务必在评论区大声指出</strong>，你的每一次“找茬”都能让它变得更聪明。</p>

<h3 id="我们关注什么">我们关注什么？</h3>

<p>这个知识库将是你的 AI 军火库，长期更新 <strong>AI Infra 全栈</strong> 的尖端弹药，包括但不限于：</p>

<ul>
  <li>机器学习系统 (Machine Learning System)</li>
  <li>大模型算法 (Large Model Algorithms)</li>
  <li>AI 加速器 (AI Accelerators)</li>
</ul>

<p>准备好，和我们一起高效成长吧！</p>

<p><em>最后，特别鸣谢我的两位灵感缪斯：Gemini &amp; Grok。</em></p>

<hr />
<h3 id="-最新动态">🚀 最新动态</h3>

<ul>
  <li>
    <p><strong>[2025-08-25]</strong> <a href="/papercache/llm/engineering/train/2025/08/25/zero-offload-democratizing-billion-scale-model-training.html">ZeRO-Offload: Democratizing Billion-Scale Model Training</a></p>
  </li>
  <li>
    <p><strong>[2025-08-25]</strong> <a href="/papercache/llm/engineering/train/2025/08/25/zero-memory-optimizations-toward-training-trillion-parameter-models.html">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></p>
  </li>
  <li>
    <p><strong>[2025-08-25]</strong> <a href="/papercache/llm/engineering/train/2025/08/25/zero-infinity-breaking-the-gpu-memory-wall-for-extreme-scale-deep-learning.html">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a></p>
  </li>
  <li>
    <p><strong>[2025-08-25]</strong> <a href="/papercache/llm/engineering/train/2025/08/25/zero-bubble-pipeline-parallelism.html">ZERO BUBBLE PIPELINE PARALLELISM</a></p>
  </li>
  <li>
    <p><strong>[2025-08-25]</strong> <a href="/papercache/llm/engineering/inference/2025/08/25/with-shared-microexponents-a-little-shifting-goes-a-long-way.html">With Shared Microexponents, A Little Shifting Goes a Long Way</a></p>
  </li>
</ul>

<hr />

<h3 id="-精选论文">📚 精选论文</h3>

<h4 id="-大语言模型-llm">🤖 大语言模型 (LLM)</h4>

<ul>
  <li>
    <p><a href="/papercache/llm/engineering/train/2025/08/25/zero-offload-democratizing-billion-scale-model-training.html">ZeRO-Offload: Democratizing Billion-Scale Model Training</a></p>
  </li>
  <li>
    <p><a href="/papercache/llm/engineering/train/2025/08/25/zero-memory-optimizations-toward-training-trillion-parameter-models.html">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></p>
  </li>
  <li>
    <p><a href="/papercache/llm/engineering/train/2025/08/25/zero-infinity-breaking-the-gpu-memory-wall-for-extreme-scale-deep-learning.html">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a></p>
  </li>
  <li>
    <p><a href="/papercache/llm/engineering/train/2025/08/25/zero-bubble-pipeline-parallelism.html">ZERO BUBBLE PIPELINE PARALLELISM</a></p>
  </li>
  <li>
    <p><a href="/papercache/llm/engineering/inference/2025/08/25/with-shared-microexponents-a-little-shifting-goes-a-long-way.html">With Shared Microexponents, A Little Shifting Goes a Long Way</a></p>
  </li>
</ul>

<h4 id="️-机器学习系统-mlsys">⚙️ 机器学习系统 (MLSys)</h4>

<ul>
  <li>
    <p><a href="/papercache/mlsys/networking/2025/08/25/ucx-an-open-source-framework-for-hpc-network-apis-and-beyond.html">UCX: An Open Source Framework for HPC Network APIs and Beyond</a></p>
  </li>
  <li>
    <p><a href="/papercache/mlsys/networking/2025/08/25/ub-mesh-a-hierarchically-localized-nd-fullmesh-datacenter-network-architecture.html">UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture</a></p>
  </li>
  <li>
    <p><a href="/papercache/mlsys/gpu/2025/08/25/stream-k-work-centric-parallel-decomposition-for-dense-matrix-matrix-multiplication-on-the-gpu.html">Stream-K: Work-centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU</a></p>
  </li>
  <li>
    <p><a href="/papercache/mlsys/networking/2025/08/25/scaling-up-memory-disaggregated-applications-with-smart.html">Scaling Up Memory Disaggregated Applications with Smart</a></p>
  </li>
  <li>
    <p><a href="/papercache/mlsys/networking/2025/08/25/scale-up-ethernet-framework-scale-up-ethernet-framework-specification.html">Scale-Up Ethernet Framework Scale-Up Ethernet Framework Specification</a></p>
  </li>
</ul>

<hr />
<blockquote>
  <p><a href="/papercache/collection.html">查看所有论文…</a></p>
</blockquote>

  </div>

</article>

      </div>
    </main></body>

</html>
