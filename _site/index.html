<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ğŸ¤– æ¬¢è¿æ¥åˆ° PaperCacheï¼ | PaperCache</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="ğŸ¤– æ¬¢è¿æ¥åˆ° PaperCacheï¼" />
<meta name="author" content="shenh10" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="æ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½ç ”ç©¶è®ºæ–‡çš„ç²¾é€‰ç¼“å­˜" />
<meta property="og:description" content="æ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½ç ”ç©¶è®ºæ–‡çš„ç²¾é€‰ç¼“å­˜" />
<link rel="canonical" href="http://localhost:4000/papercache/" />
<meta property="og:url" content="http://localhost:4000/papercache/" />
<meta property="og:site_name" content="PaperCache" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ğŸ¤– æ¬¢è¿æ¥åˆ° PaperCacheï¼" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"shenh10"},"description":"æ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½ç ”ç©¶è®ºæ–‡çš„ç²¾é€‰ç¼“å­˜","headline":"ğŸ¤– æ¬¢è¿æ¥åˆ° PaperCacheï¼","name":"PaperCache","url":"http://localhost:4000/papercache/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/papercache/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/papercache/feed.xml" title="PaperCache" /><!-- MathJax 3 Configuration -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

</head><body><header class="site-header">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/papercache/">PaperCache</a>
    <nav class="site-nav">
      <div class="trigger">
        <a class="page-link" href="/papercache/">ä¸»é¡µ</a>
        <a class="page-link" href="/papercache/collection.html">è®ºæ–‡åˆé›†</a>
        <a class="page-link" href="/papercache/about/">å…³äº</a>
      </div>
    </nav>
  </div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">ğŸ¤– æ¬¢è¿æ¥åˆ° PaperCacheï¼</h1>
  </header>

  <div class="post-content">
    <p><strong>ä¸€ä¸ªç”± AI å½“å®ä¹ ç”Ÿï¼Œæˆ‘åšè€æ¿çš„åšå®¢ã€‚</strong></p>

<p>æ²¡é”™ï¼Œè¿™é‡Œçš„æ¯ä¸€ç¯‡è®ºæ–‡ç²¾è¯»ï¼Œéƒ½ä¸æ˜¯æˆ‘äº²æ‰‹å†™çš„ï¼Œè€Œæ˜¯æˆ‘çš„èµ›åšé›‡å‘˜â€”â€”<strong>LLM</strong>â€”â€”çš„ä½œå“ã€‚æˆ‘åªè´Ÿè´£æŠ•å–‚è®ºæ–‡ï¼Œå¹¶å¶å°”åœ¨å®ƒå‡ºé”™æ—¶è¿›è¡Œä¸€äº›äººå·¥å¹²é¢„ã€‚</p>

<hr />

<h3 id="å…³äºè¿™ä¸ªåšå®¢çš„è¯ç”Ÿ">å…³äºè¿™ä¸ªåšå®¢çš„è¯ç”Ÿ</h3>

<p>ä½œä¸ºä¸€ä¸ªå¤©å¤©è¢« Paper è¿½ç€è·‘çš„å·¥ç¨‹å¸ˆï¼Œæˆ‘æ›¾æŒ£æ‰åœ¨ç¬¬äºŒè¯­è¨€å’Œè¯»ä¸å®Œçš„æ–‡çŒ®åˆ—è¡¨é‡Œã€‚åœ¨è¿™ä¸ª AGI æ—¶ä»£ï¼Œæˆ‘æ„è¯†åˆ°ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼š<strong>è®© AI å»è¯»é‚£äº›å…³äº AI çš„è®ºæ–‡ã€‚</strong></p>

<p>äºæ˜¯ï¼ŒPaperCache è¯ç”Ÿäº†ã€‚å®ƒæ˜¯æˆ‘é€šè¿‡ Prompt Engineering è°ƒæ•™å‡ºçš„ä¸€å¥—è‡ªåŠ¨åŒ–å·¥å…·çš„äº§ç‰©ã€‚</p>

<p>æˆ‘ä»¬çš„ç›®æ ‡å¾ˆæ˜ç¡®ï¼š</p>
<ul>
  <li>å¸®ä½ è¿‡æ»¤æ‰è®ºæ–‡ä¸­ 80% çš„å¤æ‚å™ªéŸ³ï¼Œè®©ä½ è¿…é€Ÿå¸æ”¶æ ¸å¿ƒæ€æƒ³ã€‚</li>
  <li>æŠŠä½ é•¿é•¿çš„ â€œç¨åè¯»â€ æ¸…å•ï¼Œå˜æˆé«˜æ•ˆçš„ â€œå·²è¯»â€ åˆ—è¡¨ã€‚</li>
</ul>

<p>å½“ç„¶ï¼Œæˆ‘çš„ AI ä¼™ä¼´æœ‰æ—¶ä¼šè¿‡äºè‡ªä¿¡ï¼Œå¯èƒ½ä¼šçŠ¯ä¸€äº›é”™è¯¯ã€‚å¦‚æœä½ å‘ç°äº†ä»»ä½• Bugï¼Œ<strong>è¯·åŠ¡å¿…åœ¨è¯„è®ºåŒºå¤§å£°æŒ‡å‡º</strong>ï¼Œä½ çš„æ¯ä¸€æ¬¡â€œæ‰¾èŒ¬â€éƒ½èƒ½è®©å®ƒå˜å¾—æ›´èªæ˜ã€‚</p>

<h3 id="æˆ‘ä»¬å…³æ³¨ä»€ä¹ˆ">æˆ‘ä»¬å…³æ³¨ä»€ä¹ˆï¼Ÿ</h3>

<p>è¿™ä¸ªçŸ¥è¯†åº“å°†æ˜¯ä½ çš„ AI å†›ç«åº“ï¼Œé•¿æœŸæ›´æ–° <strong>AI Infra å…¨æ ˆ</strong> çš„å°–ç«¯å¼¹è¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š</p>

<ul>
  <li>æœºå™¨å­¦ä¹ ç³»ç»Ÿ (Machine Learning System)</li>
  <li>å¤§æ¨¡å‹ç®—æ³• (Large Model Algorithms)</li>
  <li>AI åŠ é€Ÿå™¨ (AI Accelerators)</li>
</ul>

<p>å‡†å¤‡å¥½ï¼Œå’Œæˆ‘ä»¬ä¸€èµ·é«˜æ•ˆæˆé•¿å§ï¼</p>

<p><em>æœ€åï¼Œç‰¹åˆ«é¸£è°¢æˆ‘çš„ä¸¤ä½çµæ„Ÿç¼ªæ–¯ï¼šGemini &amp; Grokã€‚</em></p>

<hr />
<h3 id="-æœ€æ–°åŠ¨æ€">ğŸš€ æœ€æ–°åŠ¨æ€</h3>

<ul>
  <li>
    <p><strong>[2025-08-24]</strong> <a href="/papercache/mlsys/framework/2025/08/24/campo-cost-aware-performance-optimization-for-mixed-precision-neural-network-training.html">Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training</a></p>
  </li>
  <li>
    <p><strong>[2025-07-30]</strong> <a href="/papercache/llm/engineering/inference/2025/07/30/step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding.html">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a></p>
  </li>
  <li>
    <p><strong>[2025-07-30]</strong> <a href="/papercache/mlsys/networking/2025/07/30/scale-up-ethernet-framework-scale-up-ethernet-framework-specification.html">Scale-Up Ethernet Framework Scale-Up Ethernet Framework Specification</a></p>
  </li>
  <li>
    <p><strong>[2025-07-30]</strong> <a href="/papercache/llm/engineering/inference/2025/07/30/megascale-infer-serving-mixture-of-experts-at-scale-with-disaggregated-expert-parallelism.html">MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</a></p>
  </li>
  <li>
    <p><strong>[2025-07-30]</strong> <a href="/papercache/mlsys/networking/2025/07/30/demystifying-nccl-an-in-depth-analysis-of-gpu-communication-protocols-and-algorithms.html">
    Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols and
    Algorithms
  </a></p>
  </li>
</ul>

<hr />

<h3 id="-ç²¾é€‰è®ºæ–‡">ğŸ“š ç²¾é€‰è®ºæ–‡</h3>

<h4 id="-å¤§è¯­è¨€æ¨¡å‹-llm">ğŸ¤– å¤§è¯­è¨€æ¨¡å‹ (LLM)</h4>

<ul>
  <li>
    <p><a href="/papercache/llm/engineering/inference/2025/07/30/step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding.html">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a></p>
  </li>
  <li>
    <p><a href="/papercache/llm/engineering/inference/2025/07/30/megascale-infer-serving-mixture-of-experts-at-scale-with-disaggregated-expert-parallelism.html">MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</a></p>
  </li>
  <li>
    <p><a href="/papercache/llm/engineering/compiler/2025/06/30/mirage-a-multi-level-superoptimizer-for-tensor-programs.html">Mirage: A Multi-Level Superoptimizer for Tensor Programs</a></p>
  </li>
  <li>
    <p><a href="/papercache/llm/engineering/attention/2025/05/30/sageattention3-microscaling-fp4-attention-for-inference-and-an-exploration-of-8-bit-training.html">SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training</a></p>
  </li>
  <li>
    <p><a href="/papercache/llm/engineering/inference/2025/05/30/recipes-for-pre-training-llms-with-mxfp8.html">Recipes for Pre-training LLMs with MXFP8</a></p>
  </li>
</ul>

<h4 id="ï¸-æœºå™¨å­¦ä¹ ç³»ç»Ÿ-mlsys">âš™ï¸ æœºå™¨å­¦ä¹ ç³»ç»Ÿ (MLSys)</h4>

<ul>
  <li>
    <p><a href="/papercache/mlsys/framework/2025/08/24/campo-cost-aware-performance-optimization-for-mixed-precision-neural-network-training.html">Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training</a></p>
  </li>
  <li>
    <p><a href="/papercache/mlsys/networking/2025/07/30/scale-up-ethernet-framework-scale-up-ethernet-framework-specification.html">Scale-Up Ethernet Framework Scale-Up Ethernet Framework Specification</a></p>
  </li>
  <li>
    <p><a href="/papercache/mlsys/networking/2025/07/30/demystifying-nccl-an-in-depth-analysis-of-gpu-communication-protocols-and-algorithms.html">
    Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols and
    Algorithms
  </a></p>
  </li>
  <li>
    <p><a href="/papercache/mlsys/networking/2025/04/30/introducing-ualink-200g-10-specification.html">Introducing UALink 200G 1.0 Specification</a></p>
  </li>
  <li>
    <p><a href="/papercache/mlsys/networking/2025/03/30/ub-mesh-a-hierarchically-localized-nd-fullmesh-datacenter-network-architecture.html">UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture</a></p>
  </li>
</ul>

<hr />
<blockquote>
  <p><a href="/papercache/collection.html">æŸ¥çœ‹æ‰€æœ‰è®ºæ–‡â€¦</a></p>
</blockquote>

  </div>

</article>

      </div>
    </main></body>

</html>
