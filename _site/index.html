<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>PaperCache - 我的Paper收藏自留地 | PaperCache”</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="PaperCache - 我的Paper收藏自留地" />
<meta name="author" content="shenh10" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="LLM 驱动的 Paper 笔记。" />
<meta property="og:description" content="LLM 驱动的 Paper 笔记。" />
<link rel="canonical" href="https://shenh10.github.io/papercache/" />
<meta property="og:url" content="https://shenh10.github.io/papercache/" />
<meta property="og:site_name" content="PaperCache”" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="PaperCache - 我的Paper收藏自留地" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"shenh10"},"description":"LLM 驱动的 Paper 笔记。","headline":"PaperCache - 我的Paper收藏自留地","name":"PaperCache”","url":"https://shenh10.github.io/papercache/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/papercache/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://shenh10.github.io/papercache/feed.xml" title="PaperCache&quot;" /><!-- MathJax 数学公式支持 -->
  

  <!-- 统计代码 -->
  <!-- Google Analytics 4 -->

<script async src="https://www.googletagmanager.com/gtag/js?id=REPLACE_WITH_GA_ID"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'REPLACE_WITH_GA_ID');
</script>


<!-- Plausible Analytics -->


<!-- Umami Analytics -->


</head>

<body>
  <header class="site-header">
    <div class="wrapper">
      <a class="site-title" rel="author" href="/papercache/">PaperCache&quot;</a>
      
      <nav class="site-nav">
        <a href="/papercache/">首页</a>
        <a href="/papercache/about/">关于</a>
        <a href="https://github.com/shenh10/deepnotes" target="_blank">源代码</a>
      </nav>
    </div>
  </header>

  <main class="page-content" aria-label="Content">
    <div class="wrapper">
      <article class="post">

  <header class="post-header">
    <h1 class="post-title">PaperCache - 我的Paper收藏自留地</h1>
  </header>

  <div class="post-content">
    <p>欢迎来到我的博客！这是一个我突如其来的想法——日常工作中读paper这件事犹如家常便饭需要，但是碍于第二语言障碍总是无法做到高效的信息输入。但在AGI时代，继续按老办法啃文章、记笔记是一件不够高效的事情，我希望有办法可以提高知识社区的效率。据我所知，<img src="https://papers.cool/" alt="Paper Cool" /> 是一个很好的快速刷新论文的工具，但精度一篇文章我认为当前的工具还无法满足我的需求。</p>

<p>于是从prompt engineering开始，我实现了一套可以自动生成论文精读博客的工具。工具打磨了很多遍，做的不够完美，或多或少需要一点人工的参与——于是为了让这个过程更有意义，惠及更多的相关从业者及入门新人，我将它变成了这个公开的博客，使得知识不仅可以积累、也可以传播、查询以及讨论。</p>

<p>我希望通过阅读这里的 paper 博客，大家可以轻松吸取原文80%的内容，而不用再不断将paper加入checklist 让list越攒越多。AI 生成文章内容可能会有错误，希望大家可以在评论区积极指出。</p>

<p>最后，这个知识库会长期更新，主要收藏关于 AI Infra 全栈相关的知识内容，包括但不限于 <strong>Machine Learning System</strong>、<strong>大模型算法</strong>, <strong>AI加速器</strong> 等等，stay tuned!</p>

<p>最后，感谢 Gemini 与 Grok，让这一切变成了可能。</p>

<h2 id="-论文列表">📚 论文列表</h2>

<h3 id="推理优化">推理优化</h3>

<ul>
  <li>[2025-07] <a href="/papercache/llm/engineering/inference/2025/07/30/step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding.html">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a></li>
  <li>[2025-07] <a href="/papercache/llm/engineering/inference/2025/07/30/megascale-infer-serving-mixture-of-experts-at-scale-with-disaggregated-expert-parallelism.html">MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</a></li>
  <li>[2025-05] <a href="/papercache/llm/engineering/inference/2025/05/30/recipes-for-pre-training-llms-with-mxfp8.html">Recipes for Pre-training LLMs with MXFP8</a></li>
  <li>[2025-05] <a href="/papercache/llm/engineering/inference/2025/05/30/insights-into-deepseek-v3-scaling-challenges-and-reflections-on-hardware-for-ai-architectures.html">Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures</a></li>
  <li>[2025-04] <a href="/papercache/llm/engineering/inference/2025/04/30/tilus-a-virtual-machine-for-arbitrary-low-precision-gpgpu-computation-in-llm-serving.html">Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving</a></li>
  <li>[2025-03] <a href="/papercache/llm/engineering/inference/2025/03/30/eagle-3-scaling-up-inference-acceleration-of-large-language-models-via-training-time-test.html">EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test</a></li>
  <li>[2024-10] <a href="/papercache/llm/engineering/inference/2024/10/30/do-large-language-models-need-a-content-delivery-network.html">Do Large Language Models Need a Content Delivery Network?</a></li>
  <li>[2024-08] <a href="/papercache/llm/engineering/inference/2024/08/30/nanoflow-towards-optimal-large-language-model-serving-throughput.html">NanoFlow: Towards Optimal Large Language Model Serving Throughput</a></li>
  <li>[2024-07] <a href="/papercache/llm/engineering/inference/2024/07/30/sglang-efficient-execution-of-structured-language-model-programs.html">SGLang: Efficient Execution of Structured Language Model Programs</a></li>
  <li>[2024-07] <a href="/papercache/llm/engineering/inference/2024/07/30/mooncake-a-kvcache-centric-disaggregated-architecture-for-llm-serving.html">Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</a></li>
  <li>[2024-07] <a href="/papercache/llm/engineering/inference/2024/07/30/cost-efficient-large-language-model-serving-for-multi-turn-conversations-with-cachedattention.html">Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention</a></li>
  <li>[2024-06] <a href="/papercache/llm/engineering/inference/2024/06/30/medusa-simple-llm-inference-acceleration-framework-with-multiple-decoding-heads.html">MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a></li>
  <li>[2024-06] <a href="/papercache/llm/engineering/inference/2024/06/30/eagle-2-faster-inference-of-language-models-with-dynamic-draft-trees.html">EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees</a></li>
  <li>[2024-05] <a href="/papercache/llm/engineering/inference/2024/05/30/efficient-heterogeneous-large-language-model-decoding-with-model-attention-disaggregation.html">Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation</a></li>
  <li>[2024-05] <a href="/papercache/llm/engineering/inference/2024/05/30/cacheblend-fast-large-language-model-serving-for-rag-with-cached-knowledge-fusion.html">CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion</a></li>
  <li>[2024-04] <a href="/papercache/llm/engineering/inference/2024/04/30/prompt-cache-modular-attention-reuse-for-low-latency-inference.html">PROMPT CACHE: MODULAR ATTENTION REUSE FOR LOW-LATENCY INFERENCE</a></li>
  <li>[2024-01] <a href="/papercache/llm/engineering/inference/2024/01/30/fp6-llm-efficiently-serving-large-language-models-through-fp6-centric-algorithm-system-co-design.html">FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design</a></li>
  <li>[2024-01] <a href="/papercache/llm/engineering/inference/2024/01/30/eagle-speculative-sampling-requires-rethinking-feature-uncertainty.html">EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</a></li>
  <li>[2024-01] <a href="/papercache/llm/engineering/inference/2024/01/30/distserve-disaggregating-prefill-and-decoding-for-goodput-optimized-large-language-model-serving.html">DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</a></li>
  <li>[2023-10] <a href="/papercache/llm/engineering/inference/2023/10/30/cachegen-kv-cache-compression-and-streaming-for-fast-large-language-model-serving.html">CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving</a></li>
  <li>[2023-09] <a href="/papercache/llm/engineering/inference/2023/09/30/efficient-memory-management-for-large-language-model-serving-with-pagedattention.html">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li>
  <li>[2023-08] <a href="/papercache/llm/engineering/inference/2023/08/30/sarathi-efficient-llm-inference-by-piggybacking-decodes-with-chunked-prefills.html">SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills</a></li>
  <li>[2023-06] <a href="/papercache/llm/engineering/inference/2023/06/30/fp8-versus-int8-for-efficient-deep-learning-inference.html">FP8 versus INT8 for efficient deep learning inference</a></li>
  <li>[2023-06] <a href="/papercache/llm/engineering/inference/2023/06/30/awq-activation-aware-weight-quantization-for-on-device-llm-compression-and-acceleration.html">AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION FOR ON-DEVICE LLM COMPRESSION AND ACCELERATION</a></li>
  <li>[2023-05] <a href="/papercache/llm/engineering/inference/2023/05/30/integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models.html">Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</a></li>
  <li>[2023-04] <a href="/papercache/llm/engineering/inference/2023/04/30/with-shared-microexponents-a-little-shifting-goes-a-long-way.html">With Shared Microexponents, A Little Shifting Goes a Long Way</a></li>
  <li>[2022-11] <a href="/papercache/llm/engineering/inference/2022/11/30/smoothquant-accurate-and-efficient-post-training-quantization-for-large-language-models.html">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></li>
  <li>[2022-07] <a href="/papercache/llm/engineering/inference/2022/07/30/orca-a-distributed-serving-system-for-transformer-based-generative-models.html">Orca: A Distributed Serving System for Transformer-Based Generative Models</a></li>
</ul>

<h3 id="训练优化">训练优化</h3>

<h3 id="预训练方法">预训练方法</h3>

<ul>
  <li>[2022-03] <a href="/papercache/llm/algorithm/pretrain/2022/03/30/tensor-programs-v-tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer.html">Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer</a></li>
  <li>[2021-04] <a href="/papercache/llm/algorithm/pretrain/2021/04/30/roformer-enhanced-transformer-with-rotary-position-embedding.html">ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING</a></li>
</ul>

<h3 id="强化学习">强化学习</h3>

<h3 id="gpu加速与编译器优化">GPU加速与编译器优化</h3>

<ul>
  <li>[2025-05] <a href="/papercache/llm/engineering/attention/2025/05/30/sageattention3-microscaling-fp4-attention-for-inference-and-an-exploration-of-8-bit-training.html">SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training</a></li>
  <li>[2025-02] <a href="/papercache/llm/engineering/attention/2025/02/28/tree-attention-topology-aware-decoding-for-long-context-attention-on-gpu-clusters.html">TREE ATTENTION: TOPOLOGY-AWARE DECODING FOR LONG-CONTEXT ATTENTION ON GPU CLUSTERS</a></li>
  <li>[2025-02] <a href="/papercache/llm/engineering/attention/2025/02/28/native-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention.html">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></li>
  <li>[2025-02] <a href="/papercache/llm/engineering/attention/2025/02/28/moba-mixture-of-block-attention-for-long-context-llms.html">MOBA: MIXTURE OF BLOCK ATTENTION FOR LONG-CONTEXT LLMS</a></li>
  <li>[2025-01] <a href="/papercache/llm/engineering/attention/2025/01/30/minimax-01-scaling-foundation-models-with-lightning-attention.html">MiniMax-01: Scaling Foundation Models with Lightning Attention</a></li>
  <li>[2024-10] <a href="/papercache/llm/engineering/attention/2024/10/30/sageattention-accurate-8-bit-attention-for-plug-and-play-inference-acceleration.html">SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION</a></li>
  <li>[2024-07] <a href="/papercache/llm/engineering/attention/2024/07/30/flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision.html">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</a></li>
  <li>[2024-04] <a href="/papercache/llm/engineering/attention/2024/04/30/leave-no-context-behind-efficient-infinite-context-transformers-with-infini-attention.html">Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</a></li>
  <li>[2023-12] <a href="/papercache/llm/engineering/attention/2023/12/30/gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints.html">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a></li>
  <li>[2023-07] <a href="/papercache/llm/engineering/attention/2023/07/30/flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning.html">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></li>
  <li>[2022-07] <a href="/papercache/llm/engineering/attention/2022/07/30/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness.html">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
  <li>[2020-06] <a href="/papercache/llm/engineering/attention/2020/06/30/transformers-are-rnns-fast-autoregressive-transformers-with-linear-attention.html">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a></li>
  <li>[2020-02] <a href="/papercache/llm/engineering/attention/2020/02/28/low-rank-bottleneck-in-multi-head-attention-models.html">Low-Rank Bottleneck in Multi-head Attention Models</a></li>
  <li>[2019-11] <a href="/papercache/llm/engineering/attention/2019/11/30/fast-transformer-decoding-one-write-head-is-all-you-need.html">Fast Transformer Decoding: One Write-Head is All You Need</a></li>
</ul>

<h3 id="编译器与编程模型">编译器与编程模型</h3>

<ul>
  <li>[2025-06] <a href="/papercache/llm/engineering/compiler/2025/06/30/mirage-a-multi-level-superoptimizer-for-tensor-programs.html">Mirage: A Multi-Level Superoptimizer for Tensor Programs</a></li>
  <li>[2025-04] <a href="/papercache/llm/engineering/compiler/2025/04/30/tilelink-generating-efficient-compute-communication-overlapping-kernels-using-tile-centric-primitives.html">TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives</a></li>
  <li>[2025-04] <a href="/papercache/llm/engineering/compiler/2025/04/30/tilelang-a-composable-tiled-programming-model-for-ai-systems.html">TileLang: A Composable Tiled Programming Model for AI Systems</a></li>
  <li>[2024-12] <a href="/papercache/llm/engineering/compiler/2024/12/30/flex-attention-a-programming-model-for-generating-optimized-attention-kernels.html">FLEX ATTENTION: A PROGRAMMING MODEL FOR GENERATING OPTIMIZED ATTENTION KERNELS</a></li>
  <li>[2024-10] <a href="/papercache/llm/engineering/compiler/2024/10/30/flux-fast-software-based-communication-overlap-on-gpus-through-kernel-fusion.html">FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL FUSION</a></li>
</ul>

<h3 id="机器学习系统">机器学习系统</h3>

<ul>
  <li>[2025-07] <a href="/papercache/mlsys/networking/2025/07/30/scale-up-ethernet-framework-scale-up-ethernet-framework-specification.html">Scale-Up Ethernet Framework Scale-Up Ethernet Framework Specification</a></li>
  <li>[2025-07] <a href="/papercache/mlsys/networking/2025/07/30/demystifying-nccl-an-in-depth-analysis-of-gpu-communication-protocols-and-algorithms.html">Demystifying NCCL: An In-depth Analysis of GPU Communications Protocols and Algorithms</a></li>
  <li>[2025-04] <a href="/papercache/mlsys/networking/2025/04/30/introducing-ualink-200g-10-specification.html">Introducing UALink 200G 1.0 Specification</a></li>
  <li>[2025-03] <a href="/papercache/mlsys/networking/2025/03/30/ub-mesh-a-hierarchically-localized-nd-fullmesh-datacenter-network-architecture.html">UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture</a></li>
  <li>[2024-04] <a href="/papercache/mlsys/networking/2024/04/30/scaling-up-memory-disaggregated-applications-with-smart.html">Scaling Up Memory Disaggregated Applications with Smart</a></li>
  <li>[2023-07] <a href="/papercache/mlsys/networking/2023/07/30/overview-of-and-motivation-for-the-forthcoming-ultra-ethernet-consortium-specification.html">Overview of and Motivation for the Forthcoming Ultra Ethernet Consortium Specification</a></li>
  <li>[2022-11] <a href="/papercache/mlsys/networking/2022/11/30/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async.html">Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM and GPUDirect Async</a></li>
  <li>[2022-02] <a href="/papercache/mlsys/networking/2022/02/28/doubling-all2all-performance-with-nvidia-collective-communication-library-212.html">Doubling all2all Performance with NVIDIA Collective Communication Library 2.12</a></li>
  <li>[2020-08] <a href="/papercache/mlsys/networking/2020/08/30/an-in-depth-analysis-of-the-slingshot-interconnect.html">An In-Depth Analysis of the Slingshot Interconnect</a></li>
  <li>[2015-07] <a href="/papercache/mlsys/networking/2015/07/30/ucx-an-open-source-framework-for-hpc-network-apis-and-beyond.html">UCX: An Open Source Framework for HPC Network APIs and Beyond</a></li>
  <li>[2025-08] <a href="/papercache/mlsys/gpu/2025/08/23/demystifying-gpu-microarchitecture-through-microbenchmarking.html">Demystifying GPU Microarchitecture through Microbenchmarking</a></li>
  <li>[2023-01] <a href="/papercache/mlsys/gpu/2023/01/30/stream-k-work-centric-parallel-decomposition-for-dense-matrix-matrix-multiplication-on-the-gpu.html">Stream-K: Work-centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU</a></li>
  <li>[2020-02] <a href="/papercache/mlsys/gpu/2020/02/28/gpu-initiated-openshmem-correct-and-eicient-intra-kernel-networking-for-dgpus.html">GPU Initiated OpenSHMEM: Correct and Efficient Intra-Kernel Networking for dGPUs</a></li>
  <li>[2018-03] <a href="/papercache/mlsys/gpu/2018/03/30/improving-real-time-performance-with-cuda-persistent-threads-cuper-on-the-jetson-tx2.html">Improving Real-Time Performance with CUDA Persistent Threads (CuPer) on the Jetson TX2</a></li>
  <li>[2017-04] <a href="/papercache/mlsys/gpu/2017/04/30/locality-aware-cta-clustering-for-modern-gpus.html">Locality-Aware CTA Clustering for Modern GPUs</a></li>
  <li>[2016-04] <a href="/papercache/mlsys/gpu/2016/04/30/optimizing-performance-of-recurrent-neural-networks-on-gpus.html">Optimizing Performance of Recurrent Neural Networks on GPUs</a></li>
  <li>[2025-08] <a href="/papercache/mlsys/framework/2025/08/23/campo-cost-aware-performance-optimization-for-mixed-precision-neural-network-training.html">Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training</a></li>
</ul>

<hr />

<h2 id="关于">关于</h2>

<p><strong><a href="/papercache/about/">了解更多 →</a></strong></p>

<hr />

  </div>

</article>

    </div>
  </main>

  <footer class="site-footer">
    <div class="wrapper">
      <div class="footer-content">
        <div class="footer-col-1">
          <h3>PaperCache"</h3>
          <p>深度学习、机器学习和人工智能研究论文的精选缓存</p>
        </div>
        
        <div class="footer-col-2">
          <h3>链接</h3>
          <ul>
            <li><a href="https://github.com/shenh10/deepnotes">源代码仓库</a></li>
            <li><a href="/papercache/feed.xml">RSS订阅</a></li>
          </ul>
        </div>
        
        <div class="footer-col-3">
          <!-- 访问统计 -->
          
          <div class="site-stats">
            <span id="busuanzi_container_site_pv" style="display:none">
              总访问量 <span id="busuanzi_value_site_pv"></span> 次
            </span>
            <span id="busuanzi_container_site_uv" style="display:none">
              访客数 <span id="busuanzi_value_site_uv"></span> 人
            </span>
          </div>
          <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
          
        </div>
      </div>
      
      <div class="footer-bottom">
        <p>&copy; 2025 shenh10. 
           由 <a href="https://github.com/shenh10/deepnotes">Deep Notes</a> 自动生成</p>
      </div>
    </div>
  </footer>

</body>
</html>
