<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ° | PaperCacheâ€</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ°" />
<meta name="author" content="shenh10" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="LLM é©±åŠ¨çš„ Paper ç¬”è®°ã€‚" />
<meta property="og:description" content="LLM é©±åŠ¨çš„ Paper ç¬”è®°ã€‚" />
<link rel="canonical" href="https://shenh10.github.io/papercache/" />
<meta property="og:url" content="https://shenh10.github.io/papercache/" />
<meta property="og:site_name" content="PaperCacheâ€" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ°" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"shenh10"},"description":"LLM é©±åŠ¨çš„ Paper ç¬”è®°ã€‚","headline":"PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ°","name":"PaperCacheâ€","url":"https://shenh10.github.io/papercache/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/papercache/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://shenh10.github.io/papercache/feed.xml" title="PaperCache&quot;" /><!-- MathJax æ•°å­¦å…¬å¼æ”¯æŒ -->
  

  <!-- ç»Ÿè®¡ä»£ç  -->
  <!-- Google Analytics 4 -->

<script async src="https://www.googletagmanager.com/gtag/js?id=REPLACE_WITH_GA_ID"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'REPLACE_WITH_GA_ID');
</script>


<!-- Plausible Analytics -->


<!-- Umami Analytics -->


</head>

<body>
  <header class="site-header">
    <div class="wrapper">
      <a class="site-title" rel="author" href="/papercache/">PaperCache&quot;</a>
      
      <nav class="site-nav">
        <a href="/papercache/">é¦–é¡µ</a>
        <a href="/papercache/about/">å…³äº</a>
        <a href="https://github.com/shenh10/deepnotes" target="_blank">æºä»£ç </a>
      </nav>
    </div>
  </header>

  <main class="page-content" aria-label="Content">
    <div class="wrapper">
      <article class="post">

  <header class="post-header">
    <h1 class="post-title">PaperCache - æˆ‘çš„Paperæ”¶è—è‡ªç•™åœ°</h1>
  </header>

  <div class="post-content">
    <p>æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ï¼è¿™æ˜¯ä¸€ä¸ªæˆ‘çªå¦‚å…¶æ¥çš„æƒ³æ³•â€”â€”æ—¥å¸¸å·¥ä½œä¸­è¯»paperè¿™ä»¶äº‹çŠ¹å¦‚å®¶å¸¸ä¾¿é¥­éœ€è¦ï¼Œä½†æ˜¯ç¢äºç¬¬äºŒè¯­è¨€éšœç¢æ€»æ˜¯æ— æ³•åšåˆ°é«˜æ•ˆçš„ä¿¡æ¯è¾“å…¥ã€‚ä½†åœ¨AGIæ—¶ä»£ï¼Œç»§ç»­æŒ‰è€åŠæ³•å•ƒæ–‡ç« ã€è®°ç¬”è®°æ˜¯ä¸€ä»¶ä¸å¤Ÿé«˜æ•ˆçš„äº‹æƒ…ï¼Œæˆ‘å¸Œæœ›æœ‰åŠæ³•å¯ä»¥æé«˜çŸ¥è¯†ç¤¾åŒºçš„æ•ˆç‡ã€‚æ®æˆ‘æ‰€çŸ¥ï¼Œ<img src="https://papers.cool/" alt="Paper Cool" /> æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¿«é€Ÿåˆ·æ–°è®ºæ–‡çš„å·¥å…·ï¼Œä½†ç²¾åº¦ä¸€ç¯‡æ–‡ç« æˆ‘è®¤ä¸ºå½“å‰çš„å·¥å…·è¿˜æ— æ³•æ»¡è¶³æˆ‘çš„éœ€æ±‚ã€‚</p>

<p>äºæ˜¯ä»prompt engineeringå¼€å§‹ï¼Œæˆ‘å®ç°äº†ä¸€å¥—å¯ä»¥è‡ªåŠ¨ç”Ÿæˆè®ºæ–‡ç²¾è¯»åšå®¢çš„å·¥å…·ã€‚å·¥å…·æ‰“ç£¨äº†å¾ˆå¤šéï¼Œåšçš„ä¸å¤Ÿå®Œç¾ï¼Œæˆ–å¤šæˆ–å°‘éœ€è¦ä¸€ç‚¹äººå·¥çš„å‚ä¸â€”â€”äºæ˜¯ä¸ºäº†è®©è¿™ä¸ªè¿‡ç¨‹æ›´æœ‰æ„ä¹‰ï¼Œæƒ åŠæ›´å¤šçš„ç›¸å…³ä»ä¸šè€…åŠå…¥é—¨æ–°äººï¼Œæˆ‘å°†å®ƒå˜æˆäº†è¿™ä¸ªå…¬å¼€çš„åšå®¢ï¼Œä½¿å¾—çŸ¥è¯†ä¸ä»…å¯ä»¥ç§¯ç´¯ã€ä¹Ÿå¯ä»¥ä¼ æ’­ã€æŸ¥è¯¢ä»¥åŠè®¨è®ºã€‚</p>

<p>æˆ‘å¸Œæœ›é€šè¿‡é˜…è¯»è¿™é‡Œçš„ paper åšå®¢ï¼Œå¤§å®¶å¯ä»¥è½»æ¾å¸å–åŸæ–‡80%çš„å†…å®¹ï¼Œè€Œä¸ç”¨å†ä¸æ–­å°†paperåŠ å…¥checklist è®©listè¶Šæ”’è¶Šå¤šã€‚AI ç”Ÿæˆæ–‡ç« å†…å®¹å¯èƒ½ä¼šæœ‰é”™è¯¯ï¼Œå¸Œæœ›å¤§å®¶å¯ä»¥åœ¨è¯„è®ºåŒºç§¯ææŒ‡å‡ºã€‚</p>

<p>æœ€åï¼Œè¿™ä¸ªçŸ¥è¯†åº“ä¼šé•¿æœŸæ›´æ–°ï¼Œä¸»è¦æ”¶è—å…³äº AI Infra å…¨æ ˆç›¸å…³çš„çŸ¥è¯†å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äº <strong>Machine Learning System</strong>ã€<strong>å¤§æ¨¡å‹ç®—æ³•</strong>, <strong>AIåŠ é€Ÿå™¨</strong> ç­‰ç­‰ï¼Œstay tuned!</p>

<p>æœ€åï¼Œæ„Ÿè°¢ Gemini ä¸ Grokï¼Œè®©è¿™ä¸€åˆ‡å˜æˆäº†å¯èƒ½ã€‚</p>

<h2 id="-è®ºæ–‡åˆ—è¡¨">ğŸ“š è®ºæ–‡åˆ—è¡¨</h2>

<h3 id="æ¨ç†ä¼˜åŒ–">æ¨ç†ä¼˜åŒ–</h3>

<ul>
  <li>[2025-07] <a href="/papercache/llm/engineering/inference/2025/07/30/step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding.html">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a></li>
  <li>[2025-07] <a href="/papercache/llm/engineering/inference/2025/07/30/megascale-infer-serving-mixture-of-experts-at-scale-with-disaggregated-expert-parallelism.html">MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</a></li>
  <li>[2025-05] <a href="/papercache/llm/engineering/inference/2025/05/30/recipes-for-pre-training-llms-with-mxfp8.html">Recipes for Pre-training LLMs with MXFP8</a></li>
  <li>[2025-05] <a href="/papercache/llm/engineering/inference/2025/05/30/insights-into-deepseek-v3-scaling-challenges-and-reflections-on-hardware-for-ai-architectures.html">Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures</a></li>
  <li>[2025-04] <a href="/papercache/llm/engineering/inference/2025/04/30/tilus-a-virtual-machine-for-arbitrary-low-precision-gpgpu-computation-in-llm-serving.html">Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving</a></li>
  <li>[2025-03] <a href="/papercache/llm/engineering/inference/2025/03/30/eagle-3-scaling-up-inference-acceleration-of-large-language-models-via-training-time-test.html">EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test</a></li>
  <li>[2024-10] <a href="/papercache/llm/engineering/inference/2024/10/30/do-large-language-models-need-a-content-delivery-network.html">Do Large Language Models Need a Content Delivery Network?</a></li>
  <li>[2024-08] <a href="/papercache/llm/engineering/inference/2024/08/30/nanoflow-towards-optimal-large-language-model-serving-throughput.html">NanoFlow: Towards Optimal Large Language Model Serving Throughput</a></li>
  <li>[2024-07] <a href="/papercache/llm/engineering/inference/2024/07/30/sglang-efficient-execution-of-structured-language-model-programs.html">SGLang: Efficient Execution of Structured Language Model Programs</a></li>
  <li>[2024-07] <a href="/papercache/llm/engineering/inference/2024/07/30/mooncake-a-kvcache-centric-disaggregated-architecture-for-llm-serving.html">Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</a></li>
  <li>[2024-07] <a href="/papercache/llm/engineering/inference/2024/07/30/cost-efficient-large-language-model-serving-for-multi-turn-conversations-with-cachedattention.html">Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention</a></li>
  <li>[2024-06] <a href="/papercache/llm/engineering/inference/2024/06/30/medusa-simple-llm-inference-acceleration-framework-with-multiple-decoding-heads.html">MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a></li>
  <li>[2024-06] <a href="/papercache/llm/engineering/inference/2024/06/30/eagle-2-faster-inference-of-language-models-with-dynamic-draft-trees.html">EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees</a></li>
  <li>[2024-05] <a href="/papercache/llm/engineering/inference/2024/05/30/efficient-heterogeneous-large-language-model-decoding-with-model-attention-disaggregation.html">Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation</a></li>
  <li>[2024-05] <a href="/papercache/llm/engineering/inference/2024/05/30/cacheblend-fast-large-language-model-serving-for-rag-with-cached-knowledge-fusion.html">CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion</a></li>
  <li>[2024-04] <a href="/papercache/llm/engineering/inference/2024/04/30/prompt-cache-modular-attention-reuse-for-low-latency-inference.html">PROMPT CACHE: MODULAR ATTENTION REUSE FOR LOW-LATENCY INFERENCE</a></li>
  <li>[2024-01] <a href="/papercache/llm/engineering/inference/2024/01/30/fp6-llm-efficiently-serving-large-language-models-through-fp6-centric-algorithm-system-co-design.html">FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design</a></li>
  <li>[2024-01] <a href="/papercache/llm/engineering/inference/2024/01/30/eagle-speculative-sampling-requires-rethinking-feature-uncertainty.html">EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</a></li>
  <li>[2024-01] <a href="/papercache/llm/engineering/inference/2024/01/30/distserve-disaggregating-prefill-and-decoding-for-goodput-optimized-large-language-model-serving.html">DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</a></li>
  <li>[2023-10] <a href="/papercache/llm/engineering/inference/2023/10/30/cachegen-kv-cache-compression-and-streaming-for-fast-large-language-model-serving.html">CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving</a></li>
  <li>[2023-09] <a href="/papercache/llm/engineering/inference/2023/09/30/efficient-memory-management-for-large-language-model-serving-with-pagedattention.html">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li>
  <li>[2023-08] <a href="/papercache/llm/engineering/inference/2023/08/30/sarathi-efficient-llm-inference-by-piggybacking-decodes-with-chunked-prefills.html">SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills</a></li>
  <li>[2023-06] <a href="/papercache/llm/engineering/inference/2023/06/30/fp8-versus-int8-for-efficient-deep-learning-inference.html">FP8 versus INT8 for efficient deep learning inference</a></li>
  <li>[2023-06] <a href="/papercache/llm/engineering/inference/2023/06/30/awq-activation-aware-weight-quantization-for-on-device-llm-compression-and-acceleration.html">AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION FOR ON-DEVICE LLM COMPRESSION AND ACCELERATION</a></li>
  <li>[2023-05] <a href="/papercache/llm/engineering/inference/2023/05/30/integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models.html">Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</a></li>
  <li>[2023-04] <a href="/papercache/llm/engineering/inference/2023/04/30/with-shared-microexponents-a-little-shifting-goes-a-long-way.html">With Shared Microexponents, A Little Shifting Goes a Long Way</a></li>
  <li>[2022-11] <a href="/papercache/llm/engineering/inference/2022/11/30/smoothquant-accurate-and-efficient-post-training-quantization-for-large-language-models.html">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></li>
  <li>[2022-07] <a href="/papercache/llm/engineering/inference/2022/07/30/orca-a-distributed-serving-system-for-transformer-based-generative-models.html">Orca: A Distributed Serving System for Transformer-Based Generative Models</a></li>
</ul>

<h3 id="è®­ç»ƒä¼˜åŒ–">è®­ç»ƒä¼˜åŒ–</h3>

<h3 id="é¢„è®­ç»ƒæ–¹æ³•">é¢„è®­ç»ƒæ–¹æ³•</h3>

<ul>
  <li>[2022-03] <a href="/papercache/llm/algorithm/pretrain/2022/03/30/tensor-programs-v-tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer.html">Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer</a></li>
  <li>[2021-04] <a href="/papercache/llm/algorithm/pretrain/2021/04/30/roformer-enhanced-transformer-with-rotary-position-embedding.html">ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING</a></li>
</ul>

<h3 id="å¼ºåŒ–å­¦ä¹ ">å¼ºåŒ–å­¦ä¹ </h3>

<h3 id="gpuåŠ é€Ÿä¸ç¼–è¯‘å™¨ä¼˜åŒ–">GPUåŠ é€Ÿä¸ç¼–è¯‘å™¨ä¼˜åŒ–</h3>

<ul>
  <li>[2025-05] <a href="/papercache/llm/engineering/attention/2025/05/30/sageattention3-microscaling-fp4-attention-for-inference-and-an-exploration-of-8-bit-training.html">SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training</a></li>
  <li>[2025-02] <a href="/papercache/llm/engineering/attention/2025/02/28/tree-attention-topology-aware-decoding-for-long-context-attention-on-gpu-clusters.html">TREE ATTENTION: TOPOLOGY-AWARE DECODING FOR LONG-CONTEXT ATTENTION ON GPU CLUSTERS</a></li>
  <li>[2025-02] <a href="/papercache/llm/engineering/attention/2025/02/28/native-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention.html">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></li>
  <li>[2025-02] <a href="/papercache/llm/engineering/attention/2025/02/28/moba-mixture-of-block-attention-for-long-context-llms.html">MOBA: MIXTURE OF BLOCK ATTENTION FOR LONG-CONTEXT LLMS</a></li>
  <li>[2025-01] <a href="/papercache/llm/engineering/attention/2025/01/30/minimax-01-scaling-foundation-models-with-lightning-attention.html">MiniMax-01: Scaling Foundation Models with Lightning Attention</a></li>
  <li>[2024-10] <a href="/papercache/llm/engineering/attention/2024/10/30/sageattention-accurate-8-bit-attention-for-plug-and-play-inference-acceleration.html">SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION</a></li>
  <li>[2024-07] <a href="/papercache/llm/engineering/attention/2024/07/30/flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision.html">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</a></li>
  <li>[2024-04] <a href="/papercache/llm/engineering/attention/2024/04/30/leave-no-context-behind-efficient-infinite-context-transformers-with-infini-attention.html">Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</a></li>
  <li>[2023-12] <a href="/papercache/llm/engineering/attention/2023/12/30/gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints.html">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a></li>
  <li>[2023-07] <a href="/papercache/llm/engineering/attention/2023/07/30/flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning.html">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></li>
  <li>[2022-07] <a href="/papercache/llm/engineering/attention/2022/07/30/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness.html">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
  <li>[2020-06] <a href="/papercache/llm/engineering/attention/2020/06/30/transformers-are-rnns-fast-autoregressive-transformers-with-linear-attention.html">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a></li>
  <li>[2020-02] <a href="/papercache/llm/engineering/attention/2020/02/28/low-rank-bottleneck-in-multi-head-attention-models.html">Low-Rank Bottleneck in Multi-head Attention Models</a></li>
  <li>[2019-11] <a href="/papercache/llm/engineering/attention/2019/11/30/fast-transformer-decoding-one-write-head-is-all-you-need.html">Fast Transformer Decoding: One Write-Head is All You Need</a></li>
</ul>

<h3 id="ç¼–è¯‘å™¨ä¸ç¼–ç¨‹æ¨¡å‹">ç¼–è¯‘å™¨ä¸ç¼–ç¨‹æ¨¡å‹</h3>

<ul>
  <li>[2025-06] <a href="/papercache/llm/engineering/compiler/2025/06/30/mirage-a-multi-level-superoptimizer-for-tensor-programs.html">Mirage: A Multi-Level Superoptimizer for Tensor Programs</a></li>
  <li>[2025-04] <a href="/papercache/llm/engineering/compiler/2025/04/30/tilelink-generating-efficient-compute-communication-overlapping-kernels-using-tile-centric-primitives.html">TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives</a></li>
  <li>[2025-04] <a href="/papercache/llm/engineering/compiler/2025/04/30/tilelang-a-composable-tiled-programming-model-for-ai-systems.html">TileLang: A Composable Tiled Programming Model for AI Systems</a></li>
  <li>[2024-12] <a href="/papercache/llm/engineering/compiler/2024/12/30/flex-attention-a-programming-model-for-generating-optimized-attention-kernels.html">FLEX ATTENTION: A PROGRAMMING MODEL FOR GENERATING OPTIMIZED ATTENTION KERNELS</a></li>
  <li>[2024-10] <a href="/papercache/llm/engineering/compiler/2024/10/30/flux-fast-software-based-communication-overlap-on-gpus-through-kernel-fusion.html">FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL FUSION</a></li>
</ul>

<h3 id="æœºå™¨å­¦ä¹ ç³»ç»Ÿ">æœºå™¨å­¦ä¹ ç³»ç»Ÿ</h3>

<ul>
  <li>[2025-07] <a href="/papercache/mlsys/networking/2025/07/30/scale-up-ethernet-framework-scale-up-ethernet-framework-specification.html">Scale-Up Ethernet Framework Scale-Up Ethernet Framework Specification</a></li>
  <li>[2025-07] <a href="/papercache/mlsys/networking/2025/07/30/demystifying-nccl-an-in-depth-analysis-of-gpu-communication-protocols-and-algorithms.html">Demystifying NCCL: An In-depth Analysis of GPU Communications Protocols and Algorithms</a></li>
  <li>[2025-04] <a href="/papercache/mlsys/networking/2025/04/30/introducing-ualink-200g-10-specification.html">Introducing UALink 200G 1.0 Specification</a></li>
  <li>[2025-03] <a href="/papercache/mlsys/networking/2025/03/30/ub-mesh-a-hierarchically-localized-nd-fullmesh-datacenter-network-architecture.html">UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture</a></li>
  <li>[2024-04] <a href="/papercache/mlsys/networking/2024/04/30/scaling-up-memory-disaggregated-applications-with-smart.html">Scaling Up Memory Disaggregated Applications with Smart</a></li>
  <li>[2023-07] <a href="/papercache/mlsys/networking/2023/07/30/overview-of-and-motivation-for-the-forthcoming-ultra-ethernet-consortium-specification.html">Overview of and Motivation for the Forthcoming Ultra Ethernet Consortium Specification</a></li>
  <li>[2022-11] <a href="/papercache/mlsys/networking/2022/11/30/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async.html">Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM and GPUDirect Async</a></li>
  <li>[2022-02] <a href="/papercache/mlsys/networking/2022/02/28/doubling-all2all-performance-with-nvidia-collective-communication-library-212.html">Doubling all2all Performance with NVIDIA Collective Communication Library 2.12</a></li>
  <li>[2020-08] <a href="/papercache/mlsys/networking/2020/08/30/an-in-depth-analysis-of-the-slingshot-interconnect.html">An In-Depth Analysis of the Slingshot Interconnect</a></li>
  <li>[2015-07] <a href="/papercache/mlsys/networking/2015/07/30/ucx-an-open-source-framework-for-hpc-network-apis-and-beyond.html">UCX: An Open Source Framework for HPC Network APIs and Beyond</a></li>
  <li>[2025-08] <a href="/papercache/mlsys/gpu/2025/08/23/demystifying-gpu-microarchitecture-through-microbenchmarking.html">Demystifying GPU Microarchitecture through Microbenchmarking</a></li>
  <li>[2023-01] <a href="/papercache/mlsys/gpu/2023/01/30/stream-k-work-centric-parallel-decomposition-for-dense-matrix-matrix-multiplication-on-the-gpu.html">Stream-K: Work-centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU</a></li>
  <li>[2020-02] <a href="/papercache/mlsys/gpu/2020/02/28/gpu-initiated-openshmem-correct-and-eicient-intra-kernel-networking-for-dgpus.html">GPU Initiated OpenSHMEM: Correct and Efficient Intra-Kernel Networking for dGPUs</a></li>
  <li>[2018-03] <a href="/papercache/mlsys/gpu/2018/03/30/improving-real-time-performance-with-cuda-persistent-threads-cuper-on-the-jetson-tx2.html">Improving Real-Time Performance with CUDA Persistent Threads (CuPer) on the Jetson TX2</a></li>
  <li>[2017-04] <a href="/papercache/mlsys/gpu/2017/04/30/locality-aware-cta-clustering-for-modern-gpus.html">Locality-Aware CTA Clustering for Modern GPUs</a></li>
  <li>[2016-04] <a href="/papercache/mlsys/gpu/2016/04/30/optimizing-performance-of-recurrent-neural-networks-on-gpus.html">Optimizing Performance of Recurrent Neural Networks on GPUs</a></li>
  <li>[2025-08] <a href="/papercache/mlsys/framework/2025/08/23/campo-cost-aware-performance-optimization-for-mixed-precision-neural-network-training.html">Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training</a></li>
</ul>

<hr />

<h2 id="å…³äº">å…³äº</h2>

<p><strong><a href="/papercache/about/">äº†è§£æ›´å¤š â†’</a></strong></p>

<hr />

  </div>

</article>

    </div>
  </main>

  <footer class="site-footer">
    <div class="wrapper">
      <div class="footer-content">
        <div class="footer-col-1">
          <h3>PaperCache"</h3>
          <p>æ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½ç ”ç©¶è®ºæ–‡çš„ç²¾é€‰ç¼“å­˜</p>
        </div>
        
        <div class="footer-col-2">
          <h3>é“¾æ¥</h3>
          <ul>
            <li><a href="https://github.com/shenh10/deepnotes">æºä»£ç ä»“åº“</a></li>
            <li><a href="/papercache/feed.xml">RSSè®¢é˜…</a></li>
          </ul>
        </div>
        
        <div class="footer-col-3">
          <!-- è®¿é—®ç»Ÿè®¡ -->
          
          <div class="site-stats">
            <span id="busuanzi_container_site_pv" style="display:none">
              æ€»è®¿é—®é‡ <span id="busuanzi_value_site_pv"></span> æ¬¡
            </span>
            <span id="busuanzi_container_site_uv" style="display:none">
              è®¿å®¢æ•° <span id="busuanzi_value_site_uv"></span> äºº
            </span>
          </div>
          <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
          
        </div>
      </div>
      
      <div class="footer-bottom">
        <p>&copy; 2025 shenh10. 
           ç”± <a href="https://github.com/shenh10/deepnotes">Deep Notes</a> è‡ªåŠ¨ç”Ÿæˆ</p>
      </div>
    </div>
  </footer>

</body>
</html>
