<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>🤖 欢迎来到 PaperCache！ | PaperCache</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="🤖 欢迎来到 PaperCache！" />
<meta name="author" content="shenh10" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="深度学习、机器学习和人工智能研究论文的精选缓存" />
<meta property="og:description" content="深度学习、机器学习和人工智能研究论文的精选缓存" />
<link rel="canonical" href="http://localhost:4000/papercache/" />
<meta property="og:url" content="http://localhost:4000/papercache/" />
<meta property="og:site_name" content="PaperCache" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="🤖 欢迎来到 PaperCache！" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"shenh10"},"description":"深度学习、机器学习和人工智能研究论文的精选缓存","headline":"🤖 欢迎来到 PaperCache！","name":"PaperCache","url":"http://localhost:4000/papercache/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/papercache/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/papercache/feed.xml" title="PaperCache" /><!-- MathJax 3 Configuration --><!-- Busuanzi Analytics -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/papercache/">PaperCache</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/papercache/">首页</a><a class="page-link" href="/papercache/collection.html">论文合集</a><a class="page-link" href="/papercache/about/">关于</a><a class="page-link" href="/papercache/">首页</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">🤖 欢迎来到 PaperCache！</h1>
  </header>

  <div class="post-content">
    <p><strong>一个由 AI 当实习生，我做老板的博客。</strong></p>

<p>没错，这里的每一篇论文精读，都不是我亲手写的，而是我的赛博雇员——<strong>LLM</strong>——的作品。我只负责投喂论文，并偶尔在它出错时进行一些人工干预。</p>

<hr />

<h3 id="关于这个博客的诞生">关于这个博客的诞生</h3>

<p>作为一个天天被 Paper 追着跑的工程师，我曾挣扎在第二语言和读不完的文献列表里。在这个 AGI 时代，我意识到一个简单有效的方法：<strong>让 AI 去读那些关于 AI 的论文。</strong></p>

<p>于是，PaperCache 诞生了。它是我通过 Prompt Engineering 调教出的一套自动化工具的产物。</p>

<p>我们的目标很明确：</p>
<ul>
  <li>帮你过滤掉论文中 80% 的复杂噪音，让你迅速吸收核心思想。</li>
  <li>把你长长的 “稍后读” 清单，变成高效的 “已读” 列表。</li>
</ul>

<p>当然，我的 AI 伙伴有时会过于自信，可能会犯一些错误。如果你发现了任何 Bug，<strong>请务必在评论区大声指出</strong>，你的每一次“找茬”都能让它变得更聪明。</p>

<h3 id="我们关注什么">我们关注什么？</h3>

<p>这个知识库将是你的 AI 军火库，长期更新 <strong>AI Infra 全栈</strong> 的尖端弹药，包括但不限于：</p>

<ul>
  <li>机器学习系统 (Machine Learning System)</li>
  <li>大模型算法 (Large Model Algorithms)</li>
  <li>AI 加速器 (AI Accelerators)</li>
</ul>

<p>准备好，和我们一起高效成长吧！</p>

<p><em>最后，特别鸣谢我的两位灵感缪斯：Gemini &amp; Grok。</em></p>

<hr />
<h3 id="-最新动态">🚀 最新动态</h3>

<ul>
  <li>
    <p><strong>[2025-08-25]</strong> <a href="/papercache/mlsys/framework/2025/08/25/campo-cost-aware-performance-optimization-for-mixed-precision-neural-network-training.html">Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training</a></p>
  </li>
  <li>
    <p><strong>[2025-08-01]</strong> <a href="/papercache/llm/algorithm/rl/2025/08/01/seamlessflow-a-traineragent-isolation-rl-framework-achieving-bubble-free-pipelines-via-tag-scheduling.html">SeamlessFlow: A Trainer–Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling</a></p>
  </li>
  <li>
    <p><strong>[2025-08-01]</strong> <a href="/papercache/llm/algorithm/rl/2025/08/01/on-policy-rl-meets-off-policy-experts-harmonizing-supervised-fine-tuning-and-reinforcement-learning-via-dynamic-weighting.html">ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING</a></p>
  </li>
  <li>
    <p><strong>[2025-08-01]</strong> <a href="/papercache/llm/algorithm/models/2025/08/01/kimi-k2-open-agentic-intelligence.html">KIMI K2: OPEN AGENTIC INTELLIGENCE</a></p>
  </li>
  <li>
    <p><strong>[2025-07-01]</strong> <a href="/papercache/llm/engineering/inference/2025/07/01/step-3-is-large-yet-affordable-model-system-co-design-for-cost-effective-decoding.html">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a></p>
  </li>
</ul>

<hr />

<blockquote>
  <p><a href="/papercache/collection.html">查看所有论文…</a></p>
</blockquote>

  </div>

</article>

      </div>
    </main></body>

</html>
