---
layout: page
title: PaperCache - æœºå™¨å­¦ä¹ ä¸AIèµ„æº
description: æ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½ç ”ç©¶è®ºæ–‡ä¸ç¬”è®°çš„å®Œæ•´é›†åˆ
---

# æ¬¢è¿æ¥åˆ° PaperCache

ä¸€ä¸ªå…³äº**æ·±åº¦å­¦ä¹ **ã€**æœºå™¨å­¦ä¹ **å’Œ**äººå·¥æ™ºèƒ½**çš„å®Œæ•´ç¬”è®°ã€æ•™ç¨‹å’Œèµ„æºé›†åˆã€‚

## ğŸ¤– å¤§è¯­è¨€æ¨¡å‹ (LLM)

### ç®—æ³•ç ”ç©¶
- **æ¨¡å‹æ¶æ„**: Transformerå˜ä½“ã€æ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç 
- **é¢„è®­ç»ƒæ–¹æ³•**: MLMã€CLMã€å¤šä»»åŠ¡é¢„è®­ç»ƒ
- **å¼ºåŒ–å­¦ä¹ **: RLHFã€PPOã€DPOã€å¯¹é½æŠ€æœ¯

### å·¥ç¨‹å®è·µ
- **è®­ç»ƒä¼˜åŒ–**: åˆ†å¸ƒå¼è®­ç»ƒã€å†…å­˜ä¼˜åŒ–ã€æ··åˆç²¾åº¦
- **æ¨ç†åŠ é€Ÿ**: é‡åŒ–ã€å‰ªæã€çŸ¥è¯†è’¸é¦
- **ç³»ç»Ÿè®¾è®¡**: æˆæœ¬æ•ˆç›Šã€å¯æ‰©å±•æ€§ã€éƒ¨ç½²ç­–ç•¥

## ğŸ¨ æ‰©æ•£æ¨¡å‹

- **æ‰©æ•£è¿‡ç¨‹**: å‰å‘å’Œåå‘æ‰©æ•£ã€å™ªå£°è°ƒåº¦
- **åº”ç”¨é¢†åŸŸ**: å›¾åƒç”Ÿæˆã€æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€éŸ³é¢‘ç”Ÿæˆ
- **ä¼˜åŒ–æŠ€æœ¯**: é‡‡æ ·åŠ é€Ÿã€è®­ç»ƒæ•ˆç‡ã€æ¡ä»¶ç”Ÿæˆ

## âš™ï¸ æœºå™¨å­¦ä¹ ç³»ç»Ÿ

- **ç³»ç»Ÿæ¶æ„**: åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ã€èµ„æºç®¡ç†
- **æ•°æ®ç®¡ç†**: æ•°æ®æµæ°´çº¿ã€ç‰¹å¾å­˜å‚¨ã€æ•°æ®ç‰ˆæœ¬æ§åˆ¶
- **æ¨¡å‹æœåŠ¡**: æ¨¡å‹éƒ¨ç½²ã€æœåŠ¡æ¶æ„ã€ç›‘æ§å’Œå¯è§‚æµ‹æ€§

## ğŸ“š è®ºæ–‡åˆ—è¡¨

### æ¨ç†ä¼˜åŒ–
- **[2025-07] [Step 3: Large yet Affordable Model System Co-design for Cost-effective Decoding](/llm/engineering/inference/2025-07-Step-3-is-Large-yet-Affordable-Model-system-Co-design-for-Cost-effective-Decoding.html)** - æˆæœ¬æ•ˆç›Šæ¨¡å‹è§£ç çš„ç³»ç»ŸååŒè®¾è®¡
- **[2025-07] [MegaScale: Infer Serving Mixture of Experts at Scale with Disaggregated Expert Parallelism](/llm/engineering/inference/2025-07-MegaScale-Infer-Serving-Mixture-of-Experts-at-Scale-with-Disaggregated-Expert-Parallelism.html)** - å¤§è§„æ¨¡ä¸“å®¶æ··åˆæ¨¡å‹çš„æ¨ç†æœåŠ¡
- **[2025-05] [Insights into DeepSeek V3: Scaling Challenges and Reflections on Hardware for AI Architecture](/llm/engineering/inference/2025-05-Insights-into-DeepSeek-V3-Scaling-Challenges-and-Reflections-on-Hardware-for-AI-Architecture.html)** - DeepSeek V3çš„æ‰©å±•æŒ‘æˆ˜ä¸AIæ¶æ„ç¡¬ä»¶æ€è€ƒ
- **[2025-04] [Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving](/llm/engineering/inference/2025-04-Tilus-A-Virtual-Machine-for-Arbitrary-Low-Precision-GPGPU-Computation-in-LLM-Serving.html)** - LLMæœåŠ¡ä¸­ä»»æ„ä½ç²¾åº¦GPGPUè®¡ç®—çš„è™šæ‹Ÿæœº
- **[2025-04] [CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving](/llm/engineering/inference/2025-04-CacheGen-KV-Cache-Compression-and-Streaming-for-Fast-Large-Language-Model-Serving.html)** - KVç¼“å­˜å‹ç¼©å’Œæµå¼ä¼ è¾“çš„å¿«é€Ÿå¤§è¯­è¨€æ¨¡å‹æœåŠ¡
- **[2025-04] [CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion](/llm/engineering/inference/2025-04-CacheBlend-Fast-Large-Language-Model-Serving-for-RAG-with-Cached-Knowledge-Fusion.html)** - åŸºäºç¼“å­˜çŸ¥è¯†èåˆçš„å¿«é€Ÿå¤§è¯­è¨€æ¨¡å‹RAGæœåŠ¡
- **[2024-10] [Do Large Language Models Need a Content Delivery Network?](/llm/engineering/inference/2024-10-Do-Large-Language-Models-Need-a-Content-Delivery-Network.html)** - å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦éœ€è¦å†…å®¹åˆ†å‘ç½‘ç»œï¼Ÿ
- **[2023-09] [Efficient Memory Management for Large Language Model Serving with PagedAttention](/llm/engineering/inference/2023-09-Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention.html)** - åŸºäºåˆ†é¡µæ³¨æ„åŠ›çš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å†…å­˜ç®¡ç†
- **[2023-08] [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](/llm/engineering/inference/2023-08-SARATHI-Efficient-LLM-Inference-by-Piggybacking-Decodes-with-Chunked-Prefills.html)** - é€šè¿‡åˆ†å—é¢„å¡«å……æå¸¦è§£ç çš„é«˜æ•ˆLLMæ¨ç†
- **[2022-07] [Orca: A Distributed Serving System for Transformer-Based Generative Models](/llm/engineering/inference/2022-07-Orca-A-Distributed-Serving-System-for-Transformer-Based-Generative-Models.html)** - åŸºäºTransformerçš„ç”Ÿæˆæ¨¡å‹åˆ†å¸ƒå¼æœåŠ¡ç³»ç»Ÿ

### è®­ç»ƒä¼˜åŒ–
- **[2024-02] [MegaScale: Scaling Large Language Model Training to 10K GPUs](/llm/engineering/train/2024-02-MegaScale-Scaling-Large-Language-Model-Training-to.html)** - æ‰©å±•åˆ°10K GPUçš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒ
- **[2023-11] [ZERO BUBBLE PIPELINE PARALLELISM](/llm/engineering/train/2023-11-ZERO-BUBBLE-PIPELINE-PARALLELISM.html)** - é›¶æ°”æ³¡æµæ°´çº¿å¹¶è¡Œè®­ç»ƒæŠ€æœ¯

### é¢„è®­ç»ƒæ–¹æ³•
- **[2022-03] [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](/llm/algorithm/pretrain/2022-03-Tensor-Programs-V-Tuning-Large-Neural-Networks-via-Zero-Shot-Hyperparameter-Transfer.html)** - é€šè¿‡é›¶æ ·æœ¬è¶…å‚æ•°è¿ç§»è°ƒä¼˜å¤§å‹ç¥ç»ç½‘ç»œ

### å¼ºåŒ–å­¦ä¹ 
- **[2024-10] [HybridFlow: A Flexible and Efficient RLHF Framework](/llm/algorithm/RL/2024-10-HybridFlow-A-Flexible-and-Efficient-RLHF-Framework.html)** - çµæ´»é«˜æ•ˆçš„RLHFæ¡†æ¶

### GPUåŠ é€Ÿä¸ç¼–è¯‘å™¨ä¼˜åŒ–
- **[2025-06] [SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION](/llm/engineering/gpu_acceleration/2025-06-SAGEATTENTION-ACCURATE-8-BIT-ATTENTION-FOR-PLUG-AND-PLAY-INFERENCE-ACCELERATION.html)** - å³æ’å³ç”¨æ¨ç†åŠ é€Ÿçš„ç²¾ç¡®8ä½æ³¨æ„åŠ›æœºåˆ¶
- **[2025-06] [Mirage: A Multi-Level Superoptimizer for Tensor Programs](/llm/engineering/gpu_acceleration/2025-06-Mirage-A-Multi-Level-Superoptimizer-for-Tensor-Programs.html)** - å¼ é‡ç¨‹åºçš„å¤šçº§è¶…çº§ä¼˜åŒ–å™¨
- **[2024-07] [FlashAttention 3: Fast and Accurate Attention with Asynchrony and Low-precision](/llm/engineering/gpu_acceleration/2024-07-FlashAttention-3-Fast-and-Accurate-Attention-with-Asynchrony-and-Low-precision.html)** - å¼‚æ­¥å’Œä½ç²¾åº¦çš„å¿«é€Ÿç²¾ç¡®æ³¨æ„åŠ›æœºåˆ¶
- **[2022-07] [FlashAttention: Fast and Memory Efficient Exact Attention with IO-Awareness](/llm/engineering/gpu_acceleration/2022-07-FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness.html)** - å…·æœ‰IOæ„ŸçŸ¥çš„å¿«é€Ÿå†…å­˜é«˜æ•ˆç²¾ç¡®æ³¨æ„åŠ›æœºåˆ¶

### ç¼–è¯‘å™¨ä¸ç¼–ç¨‹æ¨¡å‹
- **[2025-04] [TileLang: A Composable Tiled Programming Model for AI Systems](/llm/engineering/compiler/2025-04-TileLang-A-Composable-Tiled-Programming-Model-for-AI-Systems.html)** - AIç³»ç»Ÿçš„å¯ç»„åˆç“¦ç‰‡ç¼–ç¨‹æ¨¡å‹
- **[2024-12] [FLEX ATTENTION: A PROGRAMMING MODEL FOR GENERATING OPTIMIZED ATTENTION KERNELS](/llm/engineering/compiler/2024-12-FLEX-ATTENTION-A-PROGRAMMING-MODEL-FOR-GENERATING-OPTIMIZED-ATTENTION-KERNELS.html)** - ç”Ÿæˆä¼˜åŒ–æ³¨æ„åŠ›æ ¸çš„ç¼–ç¨‹æ¨¡å‹
- **[2024-10] [FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL FUSION](/llm/engineering/compiler/2024-10-FLUX-FAST-SOFTWARE-BASED-COMMUNICATION-OVERLAP-ON-GPUS-THROUGH-KERNEL-FUSION.html)** - é€šè¿‡æ ¸èåˆåœ¨GPUä¸Šå®ç°åŸºäºè½¯ä»¶çš„å¿«é€Ÿé€šä¿¡é‡å 

### æœºå™¨å­¦ä¹ ç³»ç»Ÿ
- **[2025-07] [Demystifying NCCL: An In-depth Analysis of GPU Communications Protocols and Algorithms](/mlsys/networking/2025-07-Demystifying-NCCL-An-In-depth-Analysis-of-GPU-Communications-Protocols-and-Algorithms.html)** - æ·±å…¥åˆ†æGPUé€šä¿¡åè®®å’Œç®—æ³•
- **[2015-07] [UCX: An Open Source Framework for HPC Network APIs and Beyond](/mlsys/networking/2015-07-UCX-An-Open-Source-Framework-for-HPC-Network-APIs-and-Beyond.html)** - é«˜æ€§èƒ½è®¡ç®—ç½‘ç»œAPIåŠæ›´å¤šåŠŸèƒ½çš„å¼€æºæ¡†æ¶

---

## å…³äº

**[äº†è§£æ›´å¤š â†’](/about/)**

PaperCache æ˜¯ä¸€ä¸ªä¸ºæ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½çˆ±å¥½è€…ã€ç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›çš„ç»¼åˆèµ„æºä¸­å¿ƒã€‚

---

*æœ¬é›†åˆæŒç»­æ›´æ–°AIå’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„æœ€æ–°ç ”ç©¶å’Œè§è§£ã€‚*
